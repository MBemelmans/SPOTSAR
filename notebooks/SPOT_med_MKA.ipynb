{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import external packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba\n",
    "from numba import vectorize\n",
    "import glob # for file search\n",
    "import copy\n",
    "import os # operating system stuff\n",
    "import re # regex\n",
    "import fastparquet # fast read/write for large data structures\n",
    "import sklearn.preprocessing as pre # for data normalisation\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import rasterio.mask\n",
    "from rasterio.plot import plotting_extent\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry.point import Point\n",
    "import pyproj\n",
    "from pyproj import CRS\n",
    "from inpoly import inpoly2 # for fast inpolygon checks\n",
    "import utm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import cm as mpl_cm\n",
    "from matplotlib import colors as mcolors \n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable # for colorbar scaling\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import rc_file_defaults\n",
    "rc_file_defaults()\n",
    "# sns.set(style=None, color_codes=True)\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry.point import Point\n",
    "import datetime\n",
    "\n",
    "import configparser\n",
    "\n",
    "from cmcrameri import cm # for scientific colourmaps\n",
    "\n",
    "###########################\n",
    "# import main local package\n",
    "import SPOTSAR_main as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm.vik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ Define user INPUTS #######################\n",
    "######## please edit the values of this block only ########\n",
    "###########################################################\n",
    "\n",
    "# define hillshade file\n",
    "HS_FILE = './test_data/DEM/TDX_Merapi_WGS84_HS.tif'\n",
    "\n",
    "# define lon and lat files\n",
    "LON_FILE = './test_data/CSK_dsc/geo2/20200910.lon'\n",
    "LAT_FILE = './test_data/CSK_dsc/geo2/20200910.lat'\n",
    "\n",
    "# define parameter text file\n",
    "PARAM_FILE = './test_data/CSK_dsc/params.txt'\n",
    "\n",
    "# define map region of interest\n",
    "lon_lims = [110.425, 110.45]\n",
    "lat_lims = [-7.555, -7.535]\n",
    "\n",
    "# define colour range {min max} (min = -max)\n",
    "vmax = 3 # range of colourscale in meters\n",
    "\n",
    "# define file names for data, lon and lat\n",
    "DIRECTORY_PATH = \"./test_data/CSK_dsc/DISP_txt4/\"\n",
    "# define path to ccp and ccs files\n",
    "DIRECTORY_PATH_CCS = \"./test_data/CSK_dsc/CCS4/\"\n",
    "\n",
    "# Set the regular expression pattern to match the file names\n",
    "# PATTERN1 = r\"^c20200927_c20201113_disp_[0-9]+_[0-9]+\\.txt$\"\n",
    "# PATTERN2 = r\"^c20200926_c20201113_disp_[0-9]+_[0-9]+\\.txt$\"\n",
    "# PATTERN3 = r\"^c20200927_c20210812_disp_[0-9]+_[0-9]+\\.txt$\"\n",
    "# PATTERN4 = r\"^c20210217_c20210218_disp_[0-9]+_[0-9]+\\.txt$\"\n",
    "\n",
    "PATTERN1 = r\"^c20210406_c20210508_disp_[0-9]+_[0-9]+\\.txt$\"\n",
    "\n",
    "# Set the regular expression pattern to match the ccs file names\n",
    "# PATTERN_CCS1 = r\"^c20200927_c20201113_ccs_[0-9]+_[0-9]+$\"\n",
    "# PATTERN_CCS2 = r\"^c20200926_c20201113_ccs_[0-9]+_[0-9]+$\"\n",
    "# PATTERN_CCS3 = r\"^c20200927_c20210812_ccs_[0-9]+_[0-9]+$\"\n",
    "# PATTERN_CCS4 = r\"^c20210217_c20210218_ccs_[0-9]+_[0-9]+$\"\n",
    "\n",
    "PATTERN_CCS1 = r\"^c20210406_c20210508_ccs_[0-9]+_[0-9]+$\"\n",
    "\n",
    "\n",
    "\n",
    "# open hillshade file and re-order offset and CCS files\n",
    "\n",
    "# open hill shade file with rasterio\n",
    "DEM_HS = rio.open(HS_FILE)\n",
    "SHADING = DEM_HS.read(1,masked=True) # rasterio bands are indexed from 1\n",
    "\n",
    "# extract DEM extent\n",
    "DEM_EXTENT=[DEM_HS.bounds.left,DEM_HS.bounds.right,DEM_HS.bounds.bottom,DEM_HS.bounds.top]\n",
    "\n",
    "# read parameters from text file\n",
    "config = configparser.ConfigParser()\n",
    "config.read(PARAM_FILE)\n",
    "WIDTH = int(config.get('params', 'width'))\n",
    "LINES = int(config.get('params', 'lines'))\n",
    "WIDTH_CCS = int(config.get('params', 'width_ccs'))\n",
    "LINES_CCS = int(config.get('params', 'lines_ccs'))\n",
    "R_START = int(config.get('params', 'r_start'))\n",
    "A_START = int(config.get('params', 'a_start'))\n",
    "R_STEP = int(config.get('params', 'r_step'))\n",
    "A_STEP = int(config.get('params', 'a_step'))\n",
    "HEADING = float(config.get('params', 'heading'))\n",
    "MEAN_INC = float(config.get('params', 'mean_inc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c20210406_c20210508_disp_74_38.txt', 'c20210406_c20210508_disp_148_72.txt', 'c20210406_c20210508_disp_224_108.txt']\n",
      "['c20210406_c20210508_ccs_74_38', 'c20210406_c20210508_ccs_148_72', 'c20210406_c20210508_ccs_224_108']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reorder file using Post_processing.reorder_files\n",
    "matching_files1 = sm.Post_processing.reorder_files(DIRECTORY_PATH,PATTERN1,0)\n",
    "matching_files_ccs1 = sm.Post_processing.reorder_files(DIRECTORY_PATH_CCS,PATTERN_CCS1,0)\n",
    "# matching_files2 = sm.Post_processing.reorder_files(DIRECTORY_PATH,PATTERN2,0)\n",
    "# matching_files_ccs2 = sm.Post_processing.reorder_files(DIRECTORY_PATH_CCS,PATTERN_CCS2,0)\n",
    "# matching_files3 = sm.Post_processing.reorder_files(DIRECTORY_PATH,PATTERN3,0)\n",
    "# matching_files_ccs3 = sm.Post_processing.reorder_files(DIRECTORY_PATH_CCS,PATTERN_CCS3,0)\n",
    "# matching_files4 = sm.Post_processing.reorder_files(DIRECTORY_PATH,PATTERN4,0)\n",
    "# matching_files_ccs4 = sm.Post_processing.reorder_files(DIRECTORY_PATH_CCS,PATTERN_CCS4,0)\n",
    "\n",
    "\n",
    "# test if file ordering has worked\n",
    "print(matching_files1)\n",
    "print(matching_files_ccs1)\n",
    "# print(matching_files2)\n",
    "# print(matching_files_ccs2)\n",
    "# print(matching_files3)\n",
    "# print(matching_files_ccs3)\n",
    "# print(matching_files4)\n",
    "# print(matching_files_ccs4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load data from files into class multi-kernel\n",
    "example_pairs = []\n",
    "# for (matching_files,matching_files_ccs) in zip([matching_files1,matching_files2,matching_files3,matching_files4],[matching_files_ccs1,matching_files_ccs2,matching_files_ccs3,matching_files_ccs4]):\n",
    "#     datastack = sm.Post_processing.MultiKernel(DIRECTORY_PATH,\n",
    "#                                             matching_files,\n",
    "#                                             DIRECTORY_PATH_CCS,\n",
    "#                                             matching_files_ccs,\n",
    "#                                             LAT_FILE,\n",
    "#                                             LON_FILE,\n",
    "#                                             HEADING,\n",
    "#                                             MEAN_INC,\n",
    "#                                             LINES_CCS,\n",
    "#                                             WIDTH_CCS)\n",
    "#     # We need to assign some data not stored in the disp.txt files.\n",
    "#     datastack.get_params_from_file_name()\n",
    "#     datastack.get_latlon_from_file(WIDTH)\n",
    "#     datastack.add_lat_lon_to_data(R_START,A_START)\n",
    "#     datastack.crop_stack_ccs(R_STEP,A_STEP)\n",
    "#     # the object datastack now has several attributes associated with the whole dataset (e.g., date1, date2, heading)\n",
    "#     # Next we add all the offset data (disp.txt) to the stack\n",
    "#     stacked_data = datastack.assign_data_to_stack(R_STEP,A_STEP)\n",
    "#     # The attribute 'Stack' we find a list of single-kernel objects which contain the actual offset data, ccp and ccs data and the coordinates.\n",
    "\n",
    "#     # add stack to list\n",
    "#     example_pairs.append(datastack)\n",
    "\n",
    "# for (matching_files,matching_files_ccs) in zip([matching_files1,matching_files2,matching_files3,matching_files4],[matching_files_ccs1,matching_files_ccs2,matching_files_ccs3,matching_files_ccs4]):\n",
    "datastack = sm.Post_processing.MultiKernel(DIRECTORY_PATH,\n",
    "                                        matching_files1,\n",
    "                                        DIRECTORY_PATH_CCS,\n",
    "                                        matching_files_ccs1,\n",
    "                                        LAT_FILE,\n",
    "                                        LON_FILE,\n",
    "                                        HEADING,\n",
    "                                        MEAN_INC,\n",
    "                                        LINES_CCS,\n",
    "                                        WIDTH_CCS)\n",
    "# We need to assign some data not stored in the disp.txt files.\n",
    "datastack.get_params_from_file_name()\n",
    "datastack.get_latlon_from_file(WIDTH)\n",
    "datastack.add_lat_lon_to_data(R_START,A_START)\n",
    "datastack.crop_stack_ccs(R_STEP,A_STEP)\n",
    "# the object datastack now has several attributes associated with the whole dataset (e.g., date1, date2, heading)\n",
    "# Next we add all the offset data (disp.txt) to the stack\n",
    "stacked_data = datastack.assign_data_to_stack(R_STEP,A_STEP)\n",
    "# The attribute 'Stack' we find a list of single-kernel objects which contain the actual offset data, ccp and ccs data and the coordinates.\n",
    "\n",
    "# add stack to list\n",
    "example_pairs.append(datastack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save_keys = ['R_idx','A_idx','Row_index','Col_index','Lon_off','Lat_off','R_off','A_off']\n",
    "# for key in dir(obj[0]):\n",
    "#     if ('HDBSCAN' == key[0:7]) and (key[-3:] != 'vec'):\n",
    "#         save_keys.append(key)\n",
    "#     if ('GLOSH' == key[0:5]) and  (key[-3:] != 'vec'):\n",
    "#         save_keys.append(key)\n",
    "#     if ('LOF' == key[0:3]) and  (key[-3:] != 'vec'):\n",
    "#         save_keys.append(key)\n",
    "# print(save_keys)\n",
    "# obj[0].to_hdf5(f'./test_data/CSK_dsc/hdf5_files/{obj[0].Name[0:-4]}_outlier_detected.h5',save_keys)\n",
    "objs = example_pairs[0].Stack\n",
    "\n",
    "files = [f'./test_data/CSK_dsc/hdf5_files/{example_pairs[0].Stack[0].Name[0:-4]}_outlier_detected2.h5',\n",
    "         f'./test_data/CSK_dsc/hdf5_files/{example_pairs[0].Stack[1].Name[0:-4]}_outlier_detected_rot.h5',\n",
    "         f'./test_data/CSK_dsc/hdf5_files/{example_pairs[0].Stack[2].Name[0:-4]}_outlier_detected_rot.h5',\n",
    "         f'./test_data/CSK_dsc/hdf5_files/{example_pairs[0].Stack[3].Name[0:-4]}_outlier_detected_rot.h5',\n",
    "         f'./test_data/CSK_dsc/hdf5_files/{example_pairs[0].Stack[4].Name[0:-4]}_outlier_detected_rot.h5',\n",
    "         ]\n",
    "# f = h5py.File(f'./test_data/CSK_dsc/hdf5_files/{obj.Name[0:-4]}_outlier_detected.h5')\n",
    "query_keys = ['HDBSCAN_labels_100_1', 'HDBSCAN_labels_100_10', 'HDBSCAN_labels_100_10_vec', 'HDBSCAN_labels_100_1_vec', 'HDBSCAN_labels_100_20', 'HDBSCAN_labels_100_20_vec', 'HDBSCAN_labels_100_30', 'HDBSCAN_labels_100_30_vec', 'HDBSCAN_labels_100_40', 'HDBSCAN_labels_100_40_vec', 'HDBSCAN_labels_100_50', 'HDBSCAN_labels_100_50_vec', 'HDBSCAN_labels_150_1', 'HDBSCAN_labels_150_15', 'HDBSCAN_labels_150_15_vec', 'HDBSCAN_labels_150_1_vec', 'HDBSCAN_labels_150_30', 'HDBSCAN_labels_150_30_vec', 'HDBSCAN_labels_150_45', 'HDBSCAN_labels_150_45_vec', 'HDBSCAN_labels_150_60', 'HDBSCAN_labels_150_60_vec', 'HDBSCAN_labels_150_75', 'HDBSCAN_labels_150_75_vec', 'HDBSCAN_labels_200_1', 'HDBSCAN_labels_200_100', 'HDBSCAN_labels_200_100_vec', 'HDBSCAN_labels_200_1_vec', 'HDBSCAN_labels_200_20', 'HDBSCAN_labels_200_20_vec', 'HDBSCAN_labels_200_40', 'HDBSCAN_labels_200_40_vec', 'HDBSCAN_labels_200_60', 'HDBSCAN_labels_200_60_vec', 'HDBSCAN_labels_200_80', 'HDBSCAN_labels_200_80_vec', 'HDBSCAN_labels_250_1', 'HDBSCAN_labels_250_100', 'HDBSCAN_labels_250_100_vec', 'HDBSCAN_labels_250_125', 'HDBSCAN_labels_250_125_vec', 'HDBSCAN_labels_250_1_vec', 'HDBSCAN_labels_250_25', 'HDBSCAN_labels_250_25_vec', 'HDBSCAN_labels_250_50', 'HDBSCAN_labels_250_50_vec', 'HDBSCAN_labels_250_75', 'HDBSCAN_labels_250_75_vec', 'HDBSCAN_labels_300_1', 'HDBSCAN_labels_300_120', 'HDBSCAN_labels_300_120_vec', 'HDBSCAN_labels_300_150', 'HDBSCAN_labels_300_150_vec', 'HDBSCAN_labels_300_1_vec', 'HDBSCAN_labels_300_30', 'HDBSCAN_labels_300_30_vec', 'HDBSCAN_labels_300_60', 'HDBSCAN_labels_300_60_vec', 'HDBSCAN_labels_300_90', 'HDBSCAN_labels_300_90_vec', 'HDBSCAN_labels_400_1', 'HDBSCAN_labels_400_120', 'HDBSCAN_labels_400_120_vec', 'HDBSCAN_labels_400_160', 'HDBSCAN_labels_400_160_vec', 'HDBSCAN_labels_400_1_vec', 'HDBSCAN_labels_400_200', 'HDBSCAN_labels_400_200_vec', 'HDBSCAN_labels_400_40', 'HDBSCAN_labels_400_40_vec', 'HDBSCAN_labels_400_80', 'HDBSCAN_labels_400_80_vec', 'HDBSCAN_labels_500_1', 'HDBSCAN_labels_500_100', 'HDBSCAN_labels_500_100_vec', 'HDBSCAN_labels_500_150', 'HDBSCAN_labels_500_150_vec', 'HDBSCAN_labels_500_1_vec', 'HDBSCAN_labels_500_200', 'HDBSCAN_labels_500_200_vec', 'HDBSCAN_labels_500_250', 'HDBSCAN_labels_500_250_vec', 'HDBSCAN_labels_500_50', 'HDBSCAN_labels_500_50_vec', 'HDBSCAN_labels_50_1', 'HDBSCAN_labels_50_10', 'HDBSCAN_labels_50_10_vec', 'HDBSCAN_labels_50_15', 'HDBSCAN_labels_50_15_vec', 'HDBSCAN_labels_50_1_vec', 'HDBSCAN_labels_50_20', 'HDBSCAN_labels_50_20_vec', 'HDBSCAN_labels_50_25', 'HDBSCAN_labels_50_25_vec', 'HDBSCAN_labels_50_5', 'HDBSCAN_labels_50_5_vec', 'HDBSCAN_outlier_scores_100_1', 'HDBSCAN_outlier_scores_100_10', 'HDBSCAN_outlier_scores_100_10_vec', 'HDBSCAN_outlier_scores_100_1_vec', 'HDBSCAN_outlier_scores_100_20', 'HDBSCAN_outlier_scores_100_20_vec', 'HDBSCAN_outlier_scores_100_30', 'HDBSCAN_outlier_scores_100_30_vec', 'HDBSCAN_outlier_scores_100_40', 'HDBSCAN_outlier_scores_100_40_vec', 'HDBSCAN_outlier_scores_100_50', 'HDBSCAN_outlier_scores_100_50_vec', 'HDBSCAN_outlier_scores_150_1', 'HDBSCAN_outlier_scores_150_15', 'HDBSCAN_outlier_scores_150_15_vec', 'HDBSCAN_outlier_scores_150_1_vec', 'HDBSCAN_outlier_scores_150_30', 'HDBSCAN_outlier_scores_150_30_vec', 'HDBSCAN_outlier_scores_150_45', 'HDBSCAN_outlier_scores_150_45_vec', 'HDBSCAN_outlier_scores_150_60', 'HDBSCAN_outlier_scores_150_60_vec', 'HDBSCAN_outlier_scores_150_75', 'HDBSCAN_outlier_scores_150_75_vec', 'HDBSCAN_outlier_scores_200_1', 'HDBSCAN_outlier_scores_200_100', 'HDBSCAN_outlier_scores_200_100_vec', 'HDBSCAN_outlier_scores_200_1_vec', 'HDBSCAN_outlier_scores_200_20', 'HDBSCAN_outlier_scores_200_20_vec', 'HDBSCAN_outlier_scores_200_40', 'HDBSCAN_outlier_scores_200_40_vec', 'HDBSCAN_outlier_scores_200_60', 'HDBSCAN_outlier_scores_200_60_vec', 'HDBSCAN_outlier_scores_200_80', 'HDBSCAN_outlier_scores_200_80_vec', 'HDBSCAN_outlier_scores_250_1', 'HDBSCAN_outlier_scores_250_100', 'HDBSCAN_outlier_scores_250_100_vec', 'HDBSCAN_outlier_scores_250_125', 'HDBSCAN_outlier_scores_250_125_vec', 'HDBSCAN_outlier_scores_250_1_vec', 'HDBSCAN_outlier_scores_250_25', 'HDBSCAN_outlier_scores_250_25_vec', 'HDBSCAN_outlier_scores_250_50', 'HDBSCAN_outlier_scores_250_50_vec', 'HDBSCAN_outlier_scores_250_75', 'HDBSCAN_outlier_scores_250_75_vec', 'HDBSCAN_outlier_scores_300_1', 'HDBSCAN_outlier_scores_300_120', 'HDBSCAN_outlier_scores_300_120_vec', 'HDBSCAN_outlier_scores_300_150', 'HDBSCAN_outlier_scores_300_150_vec', 'HDBSCAN_outlier_scores_300_1_vec', 'HDBSCAN_outlier_scores_300_30', 'HDBSCAN_outlier_scores_300_30_vec', 'HDBSCAN_outlier_scores_300_60', 'HDBSCAN_outlier_scores_300_60_vec', 'HDBSCAN_outlier_scores_300_90', 'HDBSCAN_outlier_scores_300_90_vec', 'HDBSCAN_outlier_scores_400_1', 'HDBSCAN_outlier_scores_400_120', 'HDBSCAN_outlier_scores_400_120_vec', 'HDBSCAN_outlier_scores_400_160', 'HDBSCAN_outlier_scores_400_160_vec', 'HDBSCAN_outlier_scores_400_1_vec', 'HDBSCAN_outlier_scores_400_200', 'HDBSCAN_outlier_scores_400_200_vec', 'HDBSCAN_outlier_scores_400_40', 'HDBSCAN_outlier_scores_400_40_vec', 'HDBSCAN_outlier_scores_400_80', 'HDBSCAN_outlier_scores_400_80_vec', 'HDBSCAN_outlier_scores_500_1', 'HDBSCAN_outlier_scores_500_100', 'HDBSCAN_outlier_scores_500_100_vec', 'HDBSCAN_outlier_scores_500_150', 'HDBSCAN_outlier_scores_500_150_vec', 'HDBSCAN_outlier_scores_500_1_vec', 'HDBSCAN_outlier_scores_500_200', 'HDBSCAN_outlier_scores_500_200_vec', 'HDBSCAN_outlier_scores_500_250', 'HDBSCAN_outlier_scores_500_250_vec', 'HDBSCAN_outlier_scores_500_50', 'HDBSCAN_outlier_scores_500_50_vec', 'HDBSCAN_outlier_scores_50_1', 'HDBSCAN_outlier_scores_50_10', 'HDBSCAN_outlier_scores_50_10_vec', 'HDBSCAN_outlier_scores_50_15', 'HDBSCAN_outlier_scores_50_15_vec', 'HDBSCAN_outlier_scores_50_1_vec', 'HDBSCAN_outlier_scores_50_20', 'HDBSCAN_outlier_scores_50_20_vec', 'HDBSCAN_outlier_scores_50_25', 'HDBSCAN_outlier_scores_50_25_vec', 'HDBSCAN_outlier_scores_50_5', 'HDBSCAN_outlier_scores_50_5_vec', 'HDBSCAN_probabilities_100_1', 'HDBSCAN_probabilities_100_10', 'HDBSCAN_probabilities_100_10_vec', 'HDBSCAN_probabilities_100_1_vec', 'HDBSCAN_probabilities_100_20', 'HDBSCAN_probabilities_100_20_vec', 'HDBSCAN_probabilities_100_30', 'HDBSCAN_probabilities_100_30_vec', 'HDBSCAN_probabilities_100_40', 'HDBSCAN_probabilities_100_40_vec', 'HDBSCAN_probabilities_100_50', 'HDBSCAN_probabilities_100_50_vec', 'HDBSCAN_probabilities_150_1', 'HDBSCAN_probabilities_150_15', 'HDBSCAN_probabilities_150_15_vec', 'HDBSCAN_probabilities_150_1_vec', 'HDBSCAN_probabilities_150_30', 'HDBSCAN_probabilities_150_30_vec', 'HDBSCAN_probabilities_150_45', 'HDBSCAN_probabilities_150_45_vec', 'HDBSCAN_probabilities_150_60', 'HDBSCAN_probabilities_150_60_vec', 'HDBSCAN_probabilities_150_75', 'HDBSCAN_probabilities_150_75_vec', 'HDBSCAN_probabilities_200_1', 'HDBSCAN_probabilities_200_100', 'HDBSCAN_probabilities_200_100_vec', 'HDBSCAN_probabilities_200_1_vec', 'HDBSCAN_probabilities_200_20', 'HDBSCAN_probabilities_200_20_vec', 'HDBSCAN_probabilities_200_40', 'HDBSCAN_probabilities_200_40_vec', 'HDBSCAN_probabilities_200_60', 'HDBSCAN_probabilities_200_60_vec', 'HDBSCAN_probabilities_200_80', 'HDBSCAN_probabilities_200_80_vec', 'HDBSCAN_probabilities_250_1', 'HDBSCAN_probabilities_250_100', 'HDBSCAN_probabilities_250_100_vec', 'HDBSCAN_probabilities_250_125', 'HDBSCAN_probabilities_250_125_vec', 'HDBSCAN_probabilities_250_1_vec', 'HDBSCAN_probabilities_250_25', 'HDBSCAN_probabilities_250_25_vec', 'HDBSCAN_probabilities_250_50', 'HDBSCAN_probabilities_250_50_vec', 'HDBSCAN_probabilities_250_75', 'HDBSCAN_probabilities_250_75_vec', 'HDBSCAN_probabilities_300_1', 'HDBSCAN_probabilities_300_120', 'HDBSCAN_probabilities_300_120_vec', 'HDBSCAN_probabilities_300_150', 'HDBSCAN_probabilities_300_150_vec', 'HDBSCAN_probabilities_300_1_vec', 'HDBSCAN_probabilities_300_30', 'HDBSCAN_probabilities_300_30_vec', 'HDBSCAN_probabilities_300_60', 'HDBSCAN_probabilities_300_60_vec', 'HDBSCAN_probabilities_300_90', 'HDBSCAN_probabilities_300_90_vec', 'HDBSCAN_probabilities_400_1', 'HDBSCAN_probabilities_400_120', 'HDBSCAN_probabilities_400_120_vec', 'HDBSCAN_probabilities_400_160', 'HDBSCAN_probabilities_400_160_vec', 'HDBSCAN_probabilities_400_1_vec', 'HDBSCAN_probabilities_400_200', 'HDBSCAN_probabilities_400_200_vec', 'HDBSCAN_probabilities_400_40', 'HDBSCAN_probabilities_400_40_vec', 'HDBSCAN_probabilities_400_80', 'HDBSCAN_probabilities_400_80_vec', 'HDBSCAN_probabilities_500_1', 'HDBSCAN_probabilities_500_100', 'HDBSCAN_probabilities_500_100_vec', 'HDBSCAN_probabilities_500_150', 'HDBSCAN_probabilities_500_150_vec', 'HDBSCAN_probabilities_500_1_vec', 'HDBSCAN_probabilities_500_200', 'HDBSCAN_probabilities_500_200_vec', 'HDBSCAN_probabilities_500_250', 'HDBSCAN_probabilities_500_250_vec', 'HDBSCAN_probabilities_500_50', 'HDBSCAN_probabilities_500_50_vec', 'HDBSCAN_probabilities_50_1', 'HDBSCAN_probabilities_50_10', 'HDBSCAN_probabilities_50_10_vec', 'HDBSCAN_probabilities_50_15', 'HDBSCAN_probabilities_50_15_vec', 'HDBSCAN_probabilities_50_1_vec', 'HDBSCAN_probabilities_50_20', 'HDBSCAN_probabilities_50_20_vec', 'HDBSCAN_probabilities_50_25', 'HDBSCAN_probabilities_50_25_vec', 'HDBSCAN_probabilities_50_5', 'HDBSCAN_probabilities_50_5_vec', 'LOF_labels_100', 'LOF_labels_100_vec', 'LOF_labels_150', 'LOF_labels_150_vec', 'LOF_labels_200', 'LOF_labels_200_vec', 'LOF_labels_250', 'LOF_labels_250_vec', 'LOF_labels_300', 'LOF_labels_300_vec', 'LOF_labels_400', 'LOF_labels_400_vec', 'LOF_labels_50', 'LOF_labels_500', 'LOF_labels_500_vec', 'LOF_labels_50_vec', 'LOF_outlier_score_100_vec', 'LOF_outlier_score_150_vec', 'LOF_outlier_score_200_vec', 'LOF_outlier_score_250_vec', 'LOF_outlier_score_300_vec', 'LOF_outlier_score_400_vec', 'LOF_outlier_score_500_vec', 'LOF_outlier_score_50_vec', 'LOF_outlier_scores_100', 'LOF_outlier_scores_150', 'LOF_outlier_scores_200', 'LOF_outlier_scores_250', 'LOF_outlier_scores_300', 'LOF_outlier_scores_400', 'LOF_outlier_scores_50', 'LOF_outlier_scores_500', 'Mag_off_med_diff_2', 'Mag_off_med_diff_2_vec', 'Mag_off_med_diff_3', 'Mag_off_med_diff_3_vec', 'Mag_off_med_diff_4', 'Mag_off_med_diff_4_vec', 'Mag_off_med_diff_5', 'Mag_off_med_diff_5_vec', 'Mag_off_med_diff_6', 'Mag_off_med_diff_6_vec', 'Mag_off_med_diff_7', 'Mag_off_med_diff_7_vec', 'Mag_off_med_diff_8', 'Mag_off_med_diff_8_vec', 'Mag_off_med_diff_9', 'Mag_off_med_diff_9_vec']\n",
    "\n",
    "for (obj,file) in zip(objs,files):\n",
    "    print(obj,file)\n",
    "    obj.from_hdf5(file,query_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A_idx', 'A_idx_vec', 'A_off', 'A_off_vec', 'A_win', 'Ccp_off', 'Ccp_off_vec', 'Ccs_off', 'Ccs_off_vec', 'Col_index', 'Col_index_vec', 'Date1', 'Date2', 'Heading', 'Lat_off', 'Lat_off_vec', 'Lon_off', 'Lon_off_vec', 'Mag', 'Mag_vec', 'Name', 'Nan_mask', 'Nan_mask_vec', 'Phase', 'Phase_vec', 'R_idx', 'R_idx_vec', 'R_off', 'R_off_vec', 'R_win', 'Row_index', 'Row_index_vec', 'SNR', 'SNR_vec', 'X_off', 'X_off_vec', 'Y_off', 'Y_off_vec', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'calc_Mag', 'calc_SNR', 'calc_local_L2', 'calc_phase', 'comp_ll_dist_matrix', 'from_hdf5', 'get_Row_Col_vec', 'get_Row_col_idx', 'get_attr_list', 'get_coords', 'get_data', 'get_data_4_wL2', 'get_dates', 'get_vec_data', 'get_window_size', 'mask_nan_data', 'prep_DBSCAN', 'prep_DBSCAN2', 'rem_nans', 'rem_outliers_DBSCAN', 'rem_outliers_GLOSH', 'rem_outliers_HDBSCAN', 'rem_outliers_median', 'reset_vecs', 'rotate_with_heading', 'run_DBSCAN', 'run_HDBSCAN', 'run_LOF', 'run_PCA', 'run_med_filt', 'to_hdf5']\n"
     ]
    }
   ],
   "source": [
    "print(dir(example_pairs[0].Stack[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.transforms as mtransforms\n",
    "\n",
    "def add_right_cax(ax, pad, width):\n",
    "    axpos = ax.get_position()\n",
    "    caxpos = mtransforms.Bbox.from_extents(\n",
    "        axpos.x1 + pad,\n",
    "        axpos.y0,\n",
    "        axpos.x1 + pad + width,\n",
    "        axpos.y1\n",
    "    )\n",
    "    cax = ax.figure.add_axes(caxpos)\n",
    "\n",
    "    return cax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib osx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib osx\n",
    "import pickle\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from sklearn import metrics\n",
    "from matplotlib.collections import LineCollection\n",
    "plt.close('all')\n",
    "\n",
    "# plotting conditions\n",
    "plotting = 0\n",
    "plotting_flag = 0\n",
    "labels = 0\n",
    "verbose= False\n",
    "\n",
    "# dataset to test\n",
    "obj = example_pairs[0].Stack[1]\n",
    "\n",
    "min_samples_facts = [0.00001, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "min_cluster_sizes = [50,100,150,200,250,300,400,500]\n",
    "filter_radius = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "n_cat = 50 # number of samples from each catergory of inlier/outlier\n",
    "n_runs = 1 # number of runs\n",
    "\n",
    "# get data\n",
    "r_win = getattr(obj, \"R_win\")\n",
    "a_win = getattr(obj, \"A_win\")\n",
    "r_idx = getattr(obj, \"R_idx\")\n",
    "a_idx = getattr(obj, \"A_idx\")\n",
    "\n",
    "# get testing data\n",
    "test_data1 = pd.read_csv(\n",
    "            f\"./test_data/CSK_dsc/{obj.Name[:-4]}_test_set.csv\", header=0\n",
    "        ).to_numpy()\n",
    "test_data2 = pd.read_csv(\n",
    "            f\"./test_data/CSK_dsc/{obj.Name[:-4]}_test_set3.csv\", header=0\n",
    "        ).to_numpy()\n",
    "\n",
    "test_data3 = pd.read_csv(\n",
    "            f\"./test_data/CSK_dsc/{obj.Name[:-4]}_test_set4.csv\", header=0\n",
    "        ).to_numpy()\n",
    "\n",
    "# select which test data to use\n",
    "test_data4 = np.row_stack((test_data1,test_data2,test_data3))\n",
    "\n",
    "test_data4 = np.unique(test_data4,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate based on category\n",
    "# n_cat = 60\n",
    "# test_data3 = test_data3[test_data3[:,3]==0]\n",
    "sel0 = test_data4[test_data4[:,2]==0]\n",
    "sel1 = test_data4[test_data4[:,2]==1]\n",
    "sel2 = test_data4[test_data4[:,2]==2]\n",
    "sel3 = test_data4[test_data4[:,2]==3]\n",
    "\n",
    "# initialize outlut lists\n",
    "auc_list = []\n",
    "fpr_list = []\n",
    "tpr_list = []\n",
    "thresh_list = []\n",
    "\n",
    "auc_mag_mat = np.empty((np.size(min_cluster_sizes),np.size(min_samples_facts),n_runs))\n",
    "auc_med_mat = np.empty((np.size(min_cluster_sizes),np.size(min_samples_facts),n_runs))\n",
    "auc_hdb_mat = np.empty((np.size(min_cluster_sizes),np.size(min_samples_facts),n_runs))\n",
    "auc_glsh_mat = np.empty((np.size(min_cluster_sizes),np.size(min_samples_facts),n_runs))\n",
    "auc_lof_mat = np.empty((np.size(min_cluster_sizes),np.size(min_samples_facts),n_runs))\n",
    "auc_comb123_mat = np.empty((np.size(min_cluster_sizes),np.size(min_samples_facts),n_runs))\n",
    "auc_comb23_mat = np.empty((np.size(min_cluster_sizes),np.size(min_samples_facts),n_runs))\n",
    "# main loop\n",
    "for run_id in range(n_runs):\n",
    "    ax_id = 0\n",
    "    # select n_cat point from each category of the testing data\n",
    "\n",
    "    test_data = np.row_stack((sel0[random.sample(range(np.shape(sel0)[0]),k=n_cat),:],\n",
    "                            sel1[random.sample(range(np.shape(sel1)[0]),k=n_cat),:],\n",
    "                            sel2[random.sample(range(np.shape(sel2)[0]),k=n_cat),:],\n",
    "                            sel3[random.sample(range(np.shape(sel3)[0]),k=n_cat),:]))\n",
    "    # run for each minimum cluster size\n",
    "    for clust_id, (min_clust_size, filt_rad) in enumerate(zip(min_cluster_sizes,filter_radius)):\n",
    "        min_samples = [np.max([1, int(np.round(min_clust_size * min_samples_fact))]) for min_samples_fact in min_samples_facts]\n",
    "        \n",
    "        for samp_id, min_sample in enumerate(min_samples):\n",
    "            \n",
    "            # go through each method and collect relevant results\n",
    "            print(run_id, ax_id)\n",
    "            HDBSCAN_labels = getattr(\n",
    "                obj, f\"HDBSCAN_probabilities_{min_clust_size}_{min_sample}\"\n",
    "            )\n",
    "            GLOSH_labels = getattr(\n",
    "                obj, f\"HDBSCAN_outlier_scores_{min_clust_size}_{min_sample}\"\n",
    "            )\n",
    "            LOF_labels = getattr(obj, f\"LOF_outlier_scores_{min_clust_size}\")\n",
    "            Mag_labels = getattr(obj, \"Mag\")\n",
    "            Med_labels = getattr(obj, f\"Mag_off_med_diff_{filt_rad}\")\n",
    "\n",
    "            # get coordinates and offsets for plotting vectors of testing data\n",
    "            all_lons=getattr(obj,'Lon_off')\n",
    "            all_lats=getattr(obj,'Lat_off')\n",
    "            all_X_off=getattr(obj,'X_off')\n",
    "            all_Y_off=getattr(obj,'Y_off')\n",
    "\n",
    "            # get testing data\n",
    "            test_labels = -1 * np.ones(np.shape(test_data[:, 1]))\n",
    "            mask = (test_data[:, 2] > 1)\n",
    "            test_labels[mask] = 1  # all inliers\n",
    "            test_lats = np.empty_like(test_labels)\n",
    "            test_lons = np.empty_like(test_labels)\n",
    "            test_X_off = np.empty_like(test_labels)\n",
    "            test_Y_off = np.empty_like(test_labels)\n",
    "            for row_id,test_vec in enumerate(test_data):\n",
    "                \n",
    "                idx = np.argwhere((r_idx == int(test_vec[0])) & (a_idx == int(test_vec[1])))\n",
    "                test_lats[row_id] = all_lats[idx[0][0], idx[0][1]]\n",
    "                test_lons[row_id] = all_lons[idx[0][0], idx[0][1]]\n",
    "                test_X_off[row_id] = all_X_off[idx[0][0], idx[0][1]]\n",
    "                test_Y_off[row_id] = all_Y_off[idx[0][0], idx[0][1]]\n",
    "            \n",
    "            if plotting == 0:\n",
    "                fig0, ax0 = plt.subplots(1,1)\n",
    "                ax0.hist(test_data[:,2],4)\n",
    "                fig1, ax = plt.subplots(1,1)\n",
    "                ax.imshow(SHADING,cmap=cm.grayC,alpha=0.5, extent=DEM_EXTENT)\n",
    "                q = ax.quiver(test_lons,test_lats,\n",
    "                                test_X_off,test_Y_off,\n",
    "                                test_data[:,2],\n",
    "                                scale=50, \n",
    "                                width = 0.008, \n",
    "                                edgecolor='black',\n",
    "                                linewidth=0.2)\n",
    "                plotting = 1\n",
    "\n",
    "            # collect results of methods for testing data points\n",
    "            Mag_test_labels = np.full(np.shape(test_labels), np.nan)\n",
    "            Med_test_labels = np.full(np.shape(test_labels), np.nan)\n",
    "            HDBSCAN_test_labels = np.full(np.shape(test_labels), np.nan)\n",
    "            GLOSH_test_labels = np.full(np.shape(test_labels), np.nan)\n",
    "            LOF_test_labels = np.full(np.shape(test_labels), np.nan)\n",
    "            for row_id, test_vec in enumerate(test_data):\n",
    "                idx = np.argwhere((r_idx == int(test_vec[0])) & (a_idx == int(test_vec[1])))\n",
    "                Mag_test_labels[row_id] = Mag_labels[idx[0][0], idx[0][1]]\n",
    "                Med_test_labels[row_id] = Med_labels[idx[0][0], idx[0][1]]\n",
    "                HDBSCAN_test_labels[row_id] = HDBSCAN_labels[idx[0][0], idx[0][1]]\n",
    "                GLOSH_test_labels[row_id] = GLOSH_labels[idx[0][0], idx[0][1]]\n",
    "                LOF_test_labels[row_id] = LOF_labels[idx[0][0], idx[0][1]]\n",
    "\n",
    "\n",
    "            ## convert metric to inlier probability ##\n",
    "\n",
    "            # magnitude filter (1 - normalised magnitude)\n",
    "            p_in_mag = 1- (Mag_test_labels-np.nanmin(Mag_labels))/(np.nanmax(Mag_labels)-np.nanmin(Mag_labels))\n",
    "\n",
    "            # Median filter (1 - max normalised median difference)\n",
    "            p_in_med = 1- (Med_test_labels/(np.nanmax(Med_labels)))\n",
    "\n",
    "            # LOF (normalised negative LOF)\n",
    "            p_in_lof = (LOF_test_labels-np.nanmin(LOF_labels))/(np.nanmax(LOF_labels)-np.nanmin(LOF_labels))\n",
    "\n",
    "            # HDBSCAN (probability of belonging to cluster (outliers have probability 0))\n",
    "            p_in_hdb = HDBSCAN_test_labels\n",
    "\n",
    "            # GLOSH (1 - outlier probability)\n",
    "            p_in_glsh = 1-GLOSH_test_labels\n",
    "\n",
    "\n",
    "            # p_in_comb123 = 1 - ((1-p_in_lof)*(1-p_in_glsh)*(1-p_in_hdb))\n",
    "            # p_in_comb23 = 1 - ((1-p_in_glsh)*(1-p_in_hdb))\n",
    "\n",
    "            p_in_comb123 = (p_in_lof + p_in_glsh + p_in_hdb)/3\n",
    "            # p_in_comb23 = (p_in_hdb + p_in_glsh)/2\n",
    "\n",
    "            p_in_comb23 = (p_in_hdb + p_in_med)/2\n",
    "            # p_in_comb23 = 1 - ((1-p_in_med)*(1-p_in_hdb))\n",
    "\n",
    "\n",
    "\n",
    "            # magnitude\n",
    "            fpr_mag, tpr_mag, thresh_mag = metrics.roc_curve(\n",
    "                test_labels[~np.isnan(p_in_mag)],\n",
    "                p_in_mag[~np.isnan(p_in_mag)],\n",
    "            )\n",
    "            print(f'thresh mag [0]: {thresh_mag[0]} {thresh_mag[-1]}')\n",
    "            auc_mag = metrics.roc_auc_score(\n",
    "                test_labels[~np.isnan(p_in_mag)],\n",
    "                p_in_mag[~np.isnan(p_in_mag)],\n",
    "            )\n",
    "\n",
    "            # magnitude\n",
    "            fpr_med, tpr_med, thresh_med = metrics.roc_curve(\n",
    "                test_labels[~np.isnan(p_in_med)],\n",
    "                p_in_med[~np.isnan(p_in_med)],\n",
    "            )\n",
    "            print(f'thresh med [0]: {thresh_med[0]} {thresh_med[-1]}')\n",
    "            auc_med = metrics.roc_auc_score(\n",
    "                test_labels[~np.isnan(p_in_med)],\n",
    "                p_in_med[~np.isnan(p_in_med)],\n",
    "            )\n",
    "            \n",
    "            # HDBSCAN\n",
    "            fpr_hdb, tpr_hdb, thresh_hdb = metrics.roc_curve(\n",
    "                test_labels[~np.isnan(p_in_hdb)],\n",
    "                p_in_hdb[~np.isnan(p_in_hdb)],\n",
    "            )\n",
    "            print(f'thresh hdb [0]: {thresh_hdb[0]} {thresh_hdb[-1]}')\n",
    "            auc_hdb = metrics.roc_auc_score(\n",
    "                test_labels[~np.isnan(p_in_hdb)],\n",
    "                p_in_hdb[~np.isnan(p_in_hdb)],\n",
    "            )\n",
    "            store_hdb_1 = (fpr_hdb, tpr_hdb, thresh_hdb,auc_hdb)\n",
    "\n",
    "            # GLOSH\n",
    "            fpr_glsh, tpr_glsh, thresh_glsh = metrics.roc_curve(\n",
    "                test_labels[~np.isnan(p_in_glsh)],\n",
    "                p_in_glsh[~np.isnan(p_in_glsh)],\n",
    "            )\n",
    "            print(f'thresh glsh [0]: {thresh_glsh[0]} {thresh_glsh[-1]}')\n",
    "            auc_glsh = metrics.roc_auc_score(\n",
    "                test_labels[~np.isnan(p_in_glsh)],\n",
    "                p_in_glsh[~np.isnan(p_in_glsh)],\n",
    "            )\n",
    "\n",
    "            # LOF\n",
    "            fpr_lof, tpr_lof, thresh_lof = metrics.roc_curve(\n",
    "                test_labels[~np.isnan(p_in_lof)],\n",
    "                p_in_lof[~np.isnan(p_in_lof)],\n",
    "            )\n",
    "            print(f'thresh lof [0]: {thresh_lof[0]} {thresh_lof[-1]}')\n",
    "            auc_lof = metrics.roc_auc_score(\n",
    "                test_labels[~np.isnan(p_in_lof)],\n",
    "                p_in_lof[~np.isnan(p_in_lof)],\n",
    "            )\n",
    "\n",
    "            # comb123\n",
    "            fpr_comb123, tpr_comb123, thresh_comb123 = metrics.roc_curve(\n",
    "                test_labels[~np.isnan(p_in_comb123)],\n",
    "                p_in_comb123[~np.isnan(p_in_comb123)],\n",
    "            )\n",
    "            print(f'thresh comb123 [0]: {thresh_comb123[0]} {thresh_comb123[-1]}')\n",
    "            auc_comb123 = metrics.roc_auc_score(\n",
    "                test_labels[~np.isnan(p_in_comb123)],\n",
    "                p_in_comb123[~np.isnan(p_in_comb123)],\n",
    "            )\n",
    "\n",
    "            # comb23\n",
    "            fpr_comb23, tpr_comb23, thresh_comb23 = metrics.roc_curve(\n",
    "                test_labels[~np.isnan(p_in_comb23)],\n",
    "                p_in_comb23[~np.isnan(p_in_comb23)],\n",
    "            )\n",
    "            print(f'thresh comb23 [0]: {thresh_comb23[0]} {thresh_comb23[-1]}')\n",
    "            auc_comb23 = metrics.roc_auc_score(\n",
    "                test_labels[~np.isnan(p_in_comb23)],\n",
    "                p_in_comb23[~np.isnan(p_in_comb23)],\n",
    "            )\n",
    "            \n",
    "\n",
    "            if verbose:\n",
    "                print(f'{ax_id} M min, max:{np.min(thresh_mag)},{np.max(thresh_mag)}')\n",
    "                print(f'{ax_id} Med min, max:{np.min(thresh_med)},{np.max(thresh_med)}')\n",
    "                print(f'{ax_id} H min, max:{np.min(thresh_hdb)},{np.max(thresh_hdb)}')\n",
    "                print(f'{ax_id} G min, max:{np.min(thresh_glsh)},{np.max(thresh_glsh)}')\n",
    "                print(f'{ax_id} L min, max:{np.min(thresh_lof)},{np.max(thresh_lof)}')\n",
    "                print(f'{ax_id} C123 min, max:{np.min(thresh_comb123)},{np.max(thresh_comb123)}')\n",
    "                print(f'{ax_id} C23 min, max:{np.min(thresh_comb23)},{np.max(thresh_comb23)}')\n",
    "\n",
    "                print(f'{ax_id} M min, max:{np.nanmin(p_in_mag)},{np.nanmax(p_in_mag)}')\n",
    "                print(f'{ax_id} Med min, max:{np.nanmin(p_in_med)},{np.nanmax(p_in_med)}')\n",
    "                print(f'{ax_id} H min, max:{np.nanmin(p_in_hdb)},{np.nanmax(p_in_hdb)}')\n",
    "                print(f'{ax_id} G min, max:{np.nanmin(p_in_glsh)},{np.nanmax(p_in_glsh)}')\n",
    "                print(f'{ax_id} L min, max:{np.nanmin(p_in_lof)},{np.nanmax(p_in_lof)}')\n",
    "                print(f'{ax_id} C123 min, max:{np.nanmin(p_in_comb123)},{np.nanmax(p_in_comb123)}')\n",
    "                print(f'{ax_id} C23 min, max:{np.nanmin(p_in_comb23)},{np.nanmax(p_in_comb23)}')\n",
    "\n",
    "            # set thresholds that are above the maximum value in the test dataset to the max value of the test dataset\n",
    "            thresh_mag[thresh_mag > np.nanmax(p_in_mag)] = np.nanmax(p_in_mag)\n",
    "            thresh_med[thresh_med > np.nanmax(p_in_med)] = np.nanmax(p_in_med)\n",
    "            thresh_hdb[thresh_hdb > np.nanmax(p_in_hdb)] = np.nanmax(p_in_hdb)\n",
    "            thresh_glsh[thresh_glsh > np.nanmax(p_in_glsh)] = np.nanmax(p_in_glsh)\n",
    "            thresh_lof[thresh_lof > np.nanmax(p_in_lof)] = np.nanmax(p_in_lof)\n",
    "            thresh_comb123[thresh_comb123 > np.nanmax(p_in_comb123)] = np.nanmax(p_in_comb123)\n",
    "            thresh_comb23[thresh_comb23 > np.nanmax(p_in_comb23)] = np.nanmax(p_in_comb23)\n",
    "            # store true/false positive rate, auc values and thresholds for plotting and calculatign averages\n",
    "            fpr_stack = [fpr_mag,fpr_med,fpr_hdb,fpr_glsh,fpr_lof,fpr_comb123,fpr_comb23]\n",
    "            tpr_stack = [tpr_mag,tpr_med,tpr_hdb,tpr_glsh,tpr_lof,tpr_comb123,tpr_comb23]\n",
    "            auc_stack = [auc_mag,auc_med,auc_hdb,auc_glsh,auc_lof,auc_comb123,auc_comb23]\n",
    "            thresh_stack = [thresh_mag,thresh_med,thresh_hdb,thresh_glsh,thresh_lof,thresh_comb123,thresh_comb23]\n",
    "            fpr_list.append(fpr_stack)\n",
    "            tpr_list.append(tpr_stack)\n",
    "            thresh_list.append(thresh_stack)\n",
    "            auc_mag_mat[clust_id,samp_id,run_id] = auc_mag\n",
    "            auc_med_mat[clust_id,samp_id,run_id] = auc_med\n",
    "            auc_hdb_mat[clust_id,samp_id,run_id] = auc_hdb\n",
    "            auc_glsh_mat[clust_id,samp_id,run_id] = auc_glsh\n",
    "            auc_lof_mat[clust_id,samp_id,run_id] = auc_lof\n",
    "            auc_comb123_mat[clust_id,samp_id,run_id] = auc_comb123\n",
    "            auc_comb23_mat[clust_id,samp_id,run_id] = auc_comb23\n",
    "\n",
    "            # increment ax_id\n",
    "            ax_id += 1\n",
    "\n",
    "\n",
    "print(np.shape(fpr_list))        \n",
    "# get average of auc values across runs\n",
    "mean_mag = np.mean(auc_mag_mat,axis=2)\n",
    "mean_med = np.mean(auc_med_mat,axis=2)\n",
    "mean_hdb = np.mean(auc_hdb_mat,axis=2)\n",
    "mean_glsh = np.mean(auc_glsh_mat,axis=2)\n",
    "mean_lof = np.mean(auc_lof_mat,axis=2)\n",
    "mean_comb123 = np.mean(auc_comb123_mat,axis=2)\n",
    "mean_comb23 = np.mean(auc_comb23_mat,axis=2)\n",
    "\n",
    "ax_id = 0\n",
    "fpr_mag_p = np.array(fpr_list)[:,0]\n",
    "fpr_med_p = np.array(fpr_list)[:,1]\n",
    "fpr_hdb_p = np.array(fpr_list)[:,2]\n",
    "fpr_glsh_p = np.array(fpr_list)[:,3]\n",
    "fpr_lof_p = np.array(fpr_list)[:,4]\n",
    "fpr_comb123_p = np.array(fpr_list)[:,5]\n",
    "fpr_comb23_p = np.array(fpr_list)[:,6]\n",
    "\n",
    "tpr_mag_p = np.array(tpr_list)[:,0]\n",
    "tpr_med_p = np.array(tpr_list)[:,1]\n",
    "tpr_hdb_p = np.array(tpr_list)[:,2]\n",
    "tpr_glsh_p = np.array(tpr_list)[:,3]\n",
    "tpr_lof_p = np.array(tpr_list)[:,4]\n",
    "tpr_comb123_p = np.array(tpr_list)[:,5]\n",
    "tpr_comb23_p = np.array(tpr_list)[:,6]\n",
    "\n",
    "thresh_mag_p = np.array(thresh_list,dtype='object')[:,0]\n",
    "thresh_med_p = np.array(thresh_list,dtype='object')[:,1]\n",
    "thresh_hdb_p = np.array(thresh_list,dtype='object')[:,2]\n",
    "thresh_glsh_p = np.array(thresh_list,dtype='object')[:,3]\n",
    "thresh_lof_p = np.array(thresh_list,dtype='object')[:,4]\n",
    "thresh_comb123_p = np.array(thresh_list,dtype='object')[:,5]\n",
    "thresh_comb23_p = np.array(thresh_list,dtype='object')[:,6]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copies of thresh_xxx_p\n",
    "import copy\n",
    "thresh_mag_p_cp = copy.deepcopy(thresh_mag_p)\n",
    "thresh_med_p_cp = copy.deepcopy(thresh_med_p)\n",
    "thresh_hdb_p_cp = copy.deepcopy(thresh_hdb_p)\n",
    "thresh_glsh_p_cp = copy.deepcopy(thresh_glsh_p)\n",
    "thresh_lof_p_cp = copy.deepcopy(thresh_lof_p)\n",
    "thresh_comb123_p_cp = copy.deepcopy(thresh_comb123_p)\n",
    "thresh_comb23_p_cp = copy.deepcopy(thresh_comb23_p)\n",
    "tpr_mag_p_cp = copy.deepcopy(tpr_mag_p)\n",
    "tpr_med_p_cp = copy.deepcopy(tpr_med_p)\n",
    "tpr_hdb_p_cp = copy.deepcopy(tpr_hdb_p)\n",
    "tpr_glsh_p_cp = copy.deepcopy(tpr_glsh_p)\n",
    "tpr_lof_p_cp = copy.deepcopy(tpr_lof_p)\n",
    "tpr_comb123_p_cp = copy.deepcopy(tpr_comb123_p)\n",
    "tpr_comb23_p_cp = copy.deepcopy(tpr_comb23_p)\n",
    "fpr_mag_p_cp = copy.deepcopy(fpr_mag_p)\n",
    "fpr_med_p_cp = copy.deepcopy(fpr_med_p)\n",
    "fpr_hdb_p_cp = copy.deepcopy(fpr_hdb_p)\n",
    "fpr_glsh_p_cp = copy.deepcopy(fpr_glsh_p)\n",
    "fpr_lof_p_cp = copy.deepcopy(fpr_lof_p)\n",
    "fpr_comb123_p_cp = copy.deepcopy(fpr_comb123_p)\n",
    "fpr_comb23_p_cp = copy.deepcopy(fpr_comb23_p)\n",
    "sizes = (np.size(min_cluster_sizes),np.size(min_samples_facts),n_runs)\n",
    "opt_thresh_mag = np.full(sizes,np.nan)\n",
    "opt_thresh_med = np.full(sizes,np.nan)\n",
    "opt_thresh_hdb = np.full(sizes,np.nan)\n",
    "opt_thresh_glsh = np.full(sizes,np.nan)\n",
    "opt_thresh_lof = np.full(sizes,np.nan)\n",
    "opt_thresh_comb123 = np.full(sizes,np.nan)\n",
    "opt_thresh_comb23 = np.full(sizes,np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fontsize=16\n",
    "plt.rcParams.update({'font.size': fontsize})\n",
    "\n",
    "plot_id = 0\n",
    "for run_id in range(n_runs):\n",
    "    ax_id = 0\n",
    "    for clust_id in range(np.size(min_cluster_sizes)):\n",
    "        for samp_id in range(np.size(min_samples_facts)):\n",
    "            print(plot_id)\n",
    "            print(f'max pos M: {np.argmax(tpr_mag_p_cp[plot_id]-fpr_mag_p_cp[plot_id])} : {thresh_mag_p_cp[plot_id][np.argmax(tpr_mag_p_cp[plot_id]-fpr_mag_p_cp[plot_id])]}')\n",
    "            print(f'max pos Med: {np.argmax(tpr_med_p_cp[plot_id]-fpr_med_p_cp[plot_id])} : {thresh_med_p_cp[plot_id][np.argmax(tpr_med_p_cp[plot_id]-fpr_med_p_cp[plot_id])]}')\n",
    "            print(f'max pos H: {np.argmax(tpr_hdb_p_cp[plot_id]-fpr_hdb_p_cp[plot_id])} : {thresh_hdb_p_cp[plot_id][np.argmax(tpr_hdb_p_cp[plot_id]-fpr_hdb_p_cp[plot_id])]}')\n",
    "            print(f'max pos G: {np.argmax(tpr_glsh_p_cp[plot_id]-fpr_glsh_p_cp[plot_id])} : {thresh_glsh_p_cp[plot_id][np.argmax(tpr_glsh_p_cp[plot_id]-fpr_glsh_p_cp[plot_id])]}')\n",
    "            print(f'max pos L: {np.argmax(tpr_lof_p_cp[plot_id]-fpr_lof_p_cp[plot_id])} : {thresh_lof_p_cp[plot_id][np.argmax(tpr_lof_p_cp[plot_id]-fpr_lof_p_cp[plot_id])]}')\n",
    "            print(f'max pos C123: {np.argmax(tpr_comb123_p_cp[plot_id]-fpr_comb123_p_cp[plot_id])} : {thresh_comb123_p_cp[plot_id][np.argmax(tpr_comb123_p_cp[plot_id]-fpr_comb123_p_cp[plot_id])]}')\n",
    "            print(f'max pos C23: {np.argmax(tpr_comb23_p_cp[plot_id]-fpr_comb23_p_cp[plot_id])} : {thresh_comb23_p_cp[plot_id][np.argmax(tpr_comb23_p_cp[plot_id]-fpr_comb23_p_cp[plot_id])]}')\n",
    "            opt_thresh_mag[clust_id,samp_id,run_id] = thresh_mag_p_cp[plot_id][np.argmax(tpr_mag_p_cp[plot_id]-fpr_mag_p_cp[plot_id])]\n",
    "            opt_thresh_med[clust_id,samp_id,run_id] = thresh_med_p_cp[plot_id][np.argmax(tpr_med_p_cp[plot_id]-fpr_med_p_cp[plot_id])]\n",
    "            opt_thresh_hdb[clust_id,samp_id,run_id] = thresh_hdb_p_cp[plot_id][np.argmax(tpr_hdb_p_cp[plot_id]-fpr_hdb_p_cp[plot_id])]\n",
    "            opt_thresh_glsh[clust_id,samp_id,run_id] = thresh_glsh_p_cp[plot_id][np.argmax(tpr_glsh_p_cp[plot_id]-fpr_glsh_p_cp[plot_id])]\n",
    "            opt_thresh_lof[clust_id,samp_id,run_id] = thresh_lof_p_cp[plot_id][np.argmax(tpr_lof_p_cp[plot_id]-fpr_lof_p_cp[plot_id])]\n",
    "            opt_thresh_comb123[clust_id,samp_id,run_id] = thresh_comb123_p_cp[plot_id][np.argmax(tpr_comb123_p_cp[plot_id]-fpr_comb123_p_cp[plot_id])]\n",
    "            opt_thresh_comb23[clust_id,samp_id,run_id] = thresh_comb23_p_cp[plot_id][np.argmax(tpr_comb23_p_cp[plot_id]-fpr_comb23_p_cp[plot_id])]\n",
    "            plot_id +=1\n",
    "# opt_thresh = [opt_thresh_mag, opt_thresh_hdb, opt_thresh_glsh, opt_thresh_lof]\n",
    "\n",
    "\n",
    "min_samp_label = [ str(i) for i in min_samples_facts]\n",
    "min_samp_label[0] = '>0'\n",
    "# min_clust_label_facts = [str(i) for i in min_clust_facts]\n",
    "min_clust_label_facts = [str(i) for i in min_cluster_sizes]\n",
    "min_clust_label = [str(i) for i in min_cluster_sizes]\n",
    "\n",
    "# print(opt_thresh_glsh[:,:,1])\n",
    "print(f'max thresh M: {np.max(opt_thresh_mag)}')\n",
    "print(f'max thresh Med: {np.max(opt_thresh_med)}')\n",
    "print(f'max thresh H: {np.max(opt_thresh_hdb)}')\n",
    "print(f'max thresh G: {np.max(opt_thresh_glsh)}')\n",
    "print(f'max thresh L: {np.max(opt_thresh_lof)}')\n",
    "print(f'max thresh C123: {np.max(opt_thresh_comb123)}')\n",
    "print(f'max thresh C23: {np.max(opt_thresh_comb23)}')\n",
    "print(f'min thresh M: {np.min(opt_thresh_mag)}')\n",
    "print(f'min thresh Med: {np.min(opt_thresh_med)}')\n",
    "print(f'min thresh H: {np.min(opt_thresh_hdb)}')\n",
    "print(f'min thresh G: {np.min(opt_thresh_glsh)}')\n",
    "print(f'min thresh L: {np.min(opt_thresh_lof)}')\n",
    "print(f'min thresh C123: {np.min(opt_thresh_comb123)}')\n",
    "print(f'min thresh C23: {np.min(opt_thresh_comb23)}')\n",
    "\n",
    "fig, ax = plt.subplots(2,3)\n",
    "\n",
    "auc0 = ax.flatten()[0].imshow(np.flipud(np.mean(auc_mag_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "auc0 = ax.flatten()[1].imshow(np.flipud(np.mean(auc_med_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "auc1 = ax.flatten()[2].imshow(np.flipud(np.mean(auc_lof_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "auc2 = ax.flatten()[3].imshow(np.flipud(np.mean(auc_hdb_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "auc3 = ax.flatten()[4].imshow(np.flipud(np.mean(auc_glsh_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "# auc4 = .flatten()ax[0,4].imshow(np.flipud(np.mean(auc_comb123_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "auc5 = ax.flatten()[5].imshow(np.flipud(np.mean(auc_comb23_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "\n",
    "ax.flatten()[0].set_title('Magnitude thresh.')\n",
    "ax.flatten()[1].set_title('Median difference')\n",
    "ax.flatten()[2].set_title('LOF')\n",
    "ax.flatten()[3].set_title('HDBSCAN')\n",
    "ax.flatten()[4].set_title('GLOSH')\n",
    "ax.flatten()[5].set_title('COMB median HDBSCAN')\n",
    "\n",
    "cax = add_right_cax(ax.flatten()[5], pad=0.02, width=0.02)\n",
    "cbar = fig.colorbar(auc5, cax=cax,label='AUC [-]')\n",
    "# fig.colorbar(auc5, pad = 0.25,label='AUC [-]')\n",
    "\n",
    "# for a in ax.ravel():\n",
    "#     a.set_xticks(np.array(range(len(min_samples_facts))),labels=min_samp_label,rotation=90)\n",
    "#     a.set_yticks(np.flipud(range(len(min_cluster_sizes))),labels=min_clust_label)\n",
    "\n",
    "ax[0,0].set_axis_off()\n",
    "\n",
    "ax[0,1].tick_params(\n",
    "    axis='x',          # changes apply to the y-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "ax[0,1].set_yticks([7,6,5,4,3,2,1,0],labels=['2','3','4','5','6','7','8','9'])\n",
    "\n",
    "ax[0,2].tick_params(\n",
    "    axis='x',          # changes apply to the y-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "ax[0,2].set_yticks(np.flipud(range(len(min_cluster_sizes))),labels=min_clust_label)\n",
    "\n",
    "for a in [ax[1,0],ax[1,1],ax[1,2]]:\n",
    "    a.set_xticks(np.array(range(len(min_samples_facts))),labels=min_samp_label,rotation=90)\n",
    "    a.set_yticks(np.flipud(range(len(min_cluster_sizes))),labels=min_clust_label)\n",
    "\n",
    "\n",
    "ax.flatten()[1].set_ylabel('filter radius [pixels]')\n",
    "ax.flatten()[2].set_ylabel('knn [-]')\n",
    "ax.flatten()[3].set_ylabel('min. cluster size [-]')\n",
    "ax.flatten()[4].set_ylabel('min. cluster size [-]')\n",
    "ax.flatten()[5].set_ylabel('min. cluster size [-]')\n",
    "\n",
    "\n",
    "ax.flatten()[3].set_xlabel('min. sample fraction')\n",
    "ax.flatten()[4].set_xlabel('min. sample fraction')\n",
    "ax.flatten()[5].set_xlabel('min. sample fraction')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,6)\n",
    "\n",
    "auc0 = ax.flatten()[0].imshow(np.flipud(np.mean(auc_mag_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "auc0 = ax.flatten()[1].imshow(np.flipud(np.mean(auc_med_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "auc1 = ax.flatten()[2].imshow(np.flipud(np.mean(auc_lof_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "auc2 = ax.flatten()[3].imshow(np.flipud(np.mean(auc_hdb_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "auc3 = ax.flatten()[4].imshow(np.flipud(np.mean(auc_glsh_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "# auc4 = .flatten()ax[0,4].imshow(np.flipud(np.mean(auc_comb123_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "auc5 = ax.flatten()[5].imshow(np.flipud(np.mean(auc_comb23_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "\n",
    "ax.flatten()[0].set_title('Magnitude thresh.')\n",
    "ax.flatten()[1].set_title('Median difference')\n",
    "ax.flatten()[2].set_title('LOF')\n",
    "ax.flatten()[3].set_title('HDBSCAN')\n",
    "ax.flatten()[4].set_title('GLOSH')\n",
    "ax.flatten()[5].set_title('COMB median HDBSCAN')\n",
    "\n",
    "cax = add_right_cax(ax.flatten()[5], pad=0.02, width=0.02)\n",
    "cbar = fig.colorbar(auc5, cax=cax,label='AUC [-]')\n",
    "\n",
    "ax[0].set_axis_off()\n",
    "ax[1].tick_params(\n",
    "    axis='x',          # changes apply to the y-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "ax[1].set_yticks([7,6,5,4,3,2,1,0],labels=['2','3','4','5','6','7','8','9'])\n",
    "ax[2].tick_params(\n",
    "    axis='x',          # changes apply to the y-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "ax[2].set_yticks(np.flipud(range(len(min_cluster_sizes))),labels=min_clust_label)\n",
    "\n",
    "for a in [ax[3],ax[4],ax[5]]:\n",
    "    a.set_xticks(np.array(range(len(min_samples_facts))),labels=min_samp_label,rotation=90)\n",
    "    a.set_yticks(np.flipud(range(len(min_cluster_sizes))),labels=min_clust_label)\n",
    "\n",
    "\n",
    "ax.flatten()[1].set_ylabel('filter radius [pixels]')\n",
    "ax.flatten()[2].set_ylabel('knn [-]')\n",
    "ax.flatten()[3].set_ylabel('minimum cluster size [-]')\n",
    "ax.flatten()[4].set_ylabel('minimum cluster size [-]')\n",
    "ax.flatten()[5].set_ylabel('minimum cluster size [-]')\n",
    "\n",
    "\n",
    "ax.flatten()[3].set_xlabel('min_sample fraction')\n",
    "ax.flatten()[4].set_xlabel('min_sample fraction')\n",
    "ax.flatten()[5].set_xlabel('min_sample fraction')\n",
    "fig.subplots_adjust(left=0.02, bottom=0, right=0.9, top=1, wspace=0.4, hspace=0)\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(4,6)\n",
    "\n",
    "# auc0 = ax[0,0].imshow(np.flipud(np.mean(auc_mag_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "# auc0 = ax[0,1].imshow(np.flipud(np.mean(auc_med_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "# auc1 = ax[0,2].imshow(np.flipud(np.mean(auc_lof_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "# auc2 = ax[0,3].imshow(np.flipud(np.mean(auc_hdb_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "# auc3 = ax[0,4].imshow(np.flipud(np.mean(auc_glsh_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "# # auc4 = ax[0,4].imshow(np.flipud(np.mean(auc_comb123_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "# auc5 = ax[0,5].imshow(np.flipud(np.mean(auc_comb23_mat,axis=2)),vmin=0.5,vmax=1)\n",
    "\n",
    "# ax[0,0].set_title('Magnitude thresh.')\n",
    "# ax[0,1].set_title('Magnitude thresh.')\n",
    "# ax[0,2].set_title('LOF')\n",
    "# ax[0,3].set_title('HDBSCAN')\n",
    "# ax[0,4].set_title('GLOSH')\n",
    "# # ax[0,4].set_title('COMB123')\n",
    "# ax[0,5].set_title('COMB23')\n",
    "\n",
    "# fig.colorbar(auc0, pad = 0.25,label='AUC [-]')\n",
    "# fig.colorbar(auc1, pad = 0.25,label='AUC [-]')\n",
    "# fig.colorbar(auc2, pad = 0.25,label='AUC [-]')\n",
    "# fig.colorbar(auc3, pad = 0.25,label='AUC [-]')\n",
    "# fig.colorbar(auc4, pad = 0.25,label='AUC [-]')\n",
    "# fig.colorbar(auc5, pad = 0.25,label='AUC [-]')\n",
    "\n",
    "# auc_std0 = ax[1,0].imshow(np.flipud(np.std(auc_mag_mat,axis=2)),vmin=0,vmax=0.1)\n",
    "# auc_std1 = ax[1,1].imshow(np.flipud(np.std(auc_med_mat,axis=2)),vmin=0,vmax=0.1)\n",
    "# auc_std2 = ax[1,2].imshow(np.flipud(np.std(auc_lof_mat,axis=2)),vmin=0,vmax=0.1)\n",
    "# auc_std3 = ax[1,3].imshow(np.flipud(np.std(auc_hdb_mat,axis=2)),vmin=0,vmax=0.1)\n",
    "# auc_std4 = ax[1,4].imshow(np.flipud(np.std(auc_glsh_mat,axis=2)),vmin=0,vmax=0.1)\n",
    "# # auc_std4 = ax[1,4].imshow(np.flipud(np.std(auc_comb123_mat,axis=2)),vmin=0,vmax=0.1)\n",
    "# auc_std5 = ax[1,5].imshow(np.flipud(np.std(auc_comb23_mat,axis=2)),vmin=0,vmax=0.1)\n",
    "\n",
    "\n",
    "\n",
    "# # fig.colorbar(auc_std0, pad = 0.25, label='AUC std. [-]')\n",
    "# # fig.colorbar(auc_std1, pad = 0.25, label='AUC std. [-]')\n",
    "# # fig.colorbar(auc_std2, pad = 0.25, label='AUC std. [-]')\n",
    "# # fig.colorbar(auc_std3, pad = 0.25, label='AUC std. [-]')\n",
    "# # fig.colorbar(auc_std4, pad = 0.25, label='AUC std. [-]')\n",
    "# fig.colorbar(auc_std5, pad = 0.25, label='AUC std. [-]')\n",
    "\n",
    "\n",
    "# thresh0 = ax[2,0].imshow(np.flipud(np.mean(opt_thresh_mag,axis=2)),vmin=0.5,vmax=1)\n",
    "# thresh1 = ax[2,1].imshow(np.flipud(np.mean(opt_thresh_med,axis=2)),vmin=0.5,vmax=1)\n",
    "# thresh2 = ax[2,2].imshow(np.flipud(np.mean(opt_thresh_lof,axis=2)),vmin=0.5,vmax=1)\n",
    "# thresh3 = ax[2,3].imshow(np.flipud(np.mean(opt_thresh_hdb,axis=2)),vmin=0.5,vmax=1)\n",
    "# thresh4 = ax[2,4].imshow(np.flipud(np.mean(opt_thresh_glsh,axis=2)),vmin=0.5,vmax=1)\n",
    "# # thresh4 = ax[2,4].imshow(np.flipud(np.mean(opt_thresh_comb123,axis=2)),vmin=0.5,vmax=1)\n",
    "# thresh5 = ax[2,5].imshow(np.flipud(np.mean(opt_thresh_comb23,axis=2)),vmin=0.5,vmax=1)\n",
    "\n",
    "# # fig.colorbar(thresh0,pad=0.25,label='inlier prob. [-]')\n",
    "# # fig.colorbar(thresh1,pad=0.25,label='inlier prob. [-]')\n",
    "# # fig.colorbar(thresh2,pad=0.25,label='inlier prob. [-]')\n",
    "# # fig.colorbar(thresh3,pad=0.25,label='inlier prob. [-]')\n",
    "# # fig.colorbar(thresh4,pad=0.25,label='inlier prob. [-]')\n",
    "# fig.colorbar(thresh5,pad=0.25,label='inlier prob. [-]')\n",
    "\n",
    "# std0 = ax[3,0].imshow(np.flipud(np.std(opt_thresh_mag,axis=2)),vmax=0.25)\n",
    "# std1 = ax[3,1].imshow(np.flipud(np.std(opt_thresh_med,axis=2)),vmax=0.25)\n",
    "# std2 = ax[3,2].imshow(np.flipud(np.std(opt_thresh_lof,axis=2)),vmax=0.25)\n",
    "# std3 = ax[3,3].imshow(np.flipud(np.std(opt_thresh_hdb,axis=2)),vmax=0.25)\n",
    "# std4 = ax[3,4].imshow(np.flipud(np.std(opt_thresh_glsh,axis=2)),vmax=0.25)\n",
    "# # std4 = ax[3,4].imshow(np.flipud(np.std(opt_thresh_comb123,axis=2)),vmax=0.25)\n",
    "# std5 = ax[3,5].imshow(np.flipud(np.std(opt_thresh_comb23,axis=2)),vmax=0.25)\n",
    "\n",
    "# # fig.colorbar(std0,pad=0.25,label='inlier prob. std. [-]')\n",
    "# # fig.colorbar(std1,pad=0.25,label='inlier prob. std. [-]')\n",
    "# # fig.colorbar(std2,pad=0.25,label='inlier prob. std. [-]')\n",
    "# # fig.colorbar(std3,pad=0.25,label='inlier prob. std. [-]')\n",
    "# # fig.colorbar(std4,pad=0.25,label='inlier prob. std. [-]')\n",
    "# fig.colorbar(std5,pad=0.25,label='inlier prob. std. [-]')\n",
    "\n",
    "\n",
    "# ax[0,0].set_ylabel('AUC\\nmin_clust | knn')\n",
    "# ax[1,0].set_ylabel('AUC std.\\nmin_clust | knn')\n",
    "# ax[2,0].set_ylabel('opt. threshold\\nmin_clust | knn')\n",
    "# ax[3,0].set_ylabel('opt. threshold std.\\nmin_clust | knn')\n",
    "\n",
    "\n",
    "# ax[3,0].set_xlabel('min_sample fraction')\n",
    "# ax[3,1].set_xlabel('min_sample fraction')\n",
    "# ax[3,2].set_xlabel('min_sample fraction')\n",
    "# ax[3,3].set_xlabel('min_sample fraction')\n",
    "# # ax[3,4].set_xlabel('min_sample fraction')\n",
    "# ax[3,4].set_xlabel('min_sample fraction')\n",
    "# ax[3,5].set_xlabel('min_sample fraction')\n",
    "\n",
    "# ax[0,0].set_ylabel('min_cluster_size')\n",
    "\n",
    "# min_samp_label = [ str(i) for i in min_samples_facts]\n",
    "# min_samp_label[0] = '>0'\n",
    "# # min_clust_label_facts = [str(i) for i in min_clust_facts]\n",
    "# min_clust_label_facts = [str(i) for i in min_cluster_sizes]\n",
    "# min_clust_label = [str(i) for i in min_cluster_sizes]\n",
    "\n",
    "# for a in ax.ravel():\n",
    "#     a.set_xticks(np.array(range(len(min_samples_facts))),labels=min_samp_label,rotation=90)\n",
    "#     a.set_yticks(np.flipud(range(len(min_cluster_sizes))),labels=min_clust_label)\n",
    "#     # secay = a.secondary_yaxis('right')\n",
    "#     # secay.set_ylabel('* N_{overlap}')\n",
    "#     # secay.set_yticks(np.flipud(range(len(min_cluster_sizes))),labels=min_clust_label_facts)\n",
    "\n",
    "\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from cmcrameri import cm\n",
    "\n",
    "import pyproj\n",
    "from pyproj import CRS\n",
    "\n",
    "def plot_vec_attr_alpha(obj,attr,step,scale,width =0.005 ,attr_lims=[0,1],qk_length=1,shading = [],dem_extent = [],lat_lims = [],lon_lims = [],alpha=0,fig_ax= []):\n",
    "    \"\"\"\n",
    "    Plots displacement as vectors in slantrange - azimuth plane and \n",
    "    assigns colour based on attribute value and limits\n",
    "\n",
    "    Args:\n",
    "        obj (class object): _description_\n",
    "        attr (str): _description_\n",
    "        step (int): _description_\n",
    "        scale (int): _description_\n",
    "        attr_lims (list of floats, optional): _description_. Defaults to [0,1]\n",
    "        qk_length (float, optional): _description_. Defaults to 1\n",
    "        shading (np.array, optional): _description_. Defaults to [] -> do not use.\n",
    "        dem_extent (list, optional): _description_. Defaults to [] -> do not use.\n",
    "        lat_lims (list, optional): _description_. Defaults to [] -> do not use.\n",
    "        lon_lims (list, optional): _description_. Defaults to [] -> do not use.\n",
    "    \"\"\"\n",
    "    # get length of 1 deg. for scalebar\n",
    "\n",
    "    distance_meters = sm.plot.get_1deg_dist()\n",
    "\n",
    "    # copy data to manipulate limits without messing with the original data\n",
    "    if attr != []:\n",
    "        attr_copy = copy.deepcopy(getattr(obj,attr))\n",
    "\n",
    "        # change limits for nicer plotting\n",
    "        alpha_map = np.ones_like(attr_copy)\n",
    "        alpha_map[attr_copy<attr_lims[0]] = alpha\n",
    "        attr_copy[attr_copy<attr_lims[0]] = attr_lims[0]\n",
    "        attr_copy[attr_copy>attr_lims[1]] = attr_lims[1]\n",
    "        \n",
    "\n",
    "    if lat_lims == []:\n",
    "        lat_lims = [np.min(obj.Lat_off_vec),np.max(obj.Lat_off_vec)]\n",
    "    \n",
    "    if lon_lims == []:\n",
    "        lon_lims = [np.min(obj.Lon_off_vec),np.max(obj.Lon_off_vec)]\n",
    "\n",
    "    # plotting\n",
    "    if fig_ax == []:\n",
    "        fig1, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "    else:\n",
    "        fig1 = fig_ax[0]\n",
    "        ax = fig_ax[1]\n",
    "        \n",
    "    if (dem_extent != []):\n",
    "        ax.imshow(shading,cmap=cm.grayC,alpha=0.5, extent=dem_extent)\n",
    "    \n",
    "    if attr == []:\n",
    "        q = ax.quiver(obj.Lon_off[::step,::step],obj.Lat_off[::step,::step],\n",
    "                    obj.X_off[::step,::step],obj.Y_off[::step,::step],\n",
    "                    color='black',\n",
    "                    alpha=alpha_map[::step,::step],\n",
    "                    scale=scale, \n",
    "                    width = width, \n",
    "                    edgecolor='black',\n",
    "                    linewidth=0.2,)\n",
    "    else:\n",
    "        q = ax.quiver(obj.Lon_off[::step,::step],obj.Lat_off[::step,::step],\n",
    "                        obj.X_off[::step,::step],obj.Y_off[::step,::step],\n",
    "                        attr_copy[::step,::step],\n",
    "                        alpha=alpha_map[::step,::step],\n",
    "                        scale=scale, \n",
    "                        width = width, \n",
    "                        edgecolor='black',\n",
    "                        linewidth=0.2,\n",
    "                        clim=attr_lims)\n",
    "    ax.set_ylim(lat_lims)\n",
    "    ax.set_xlim(lon_lims)\n",
    "    ax.add_artist(ScaleBar(distance_meters,location='lower right'))\n",
    "    # ax.add_artist(ScaleBar(distance_meters,location='lower right',font_properties={'size': 20}))\n",
    "    # fig1.colorbar(q,ax=axes,extend='both')\n",
    "    # cax = add_right_cax(ax, pad=0.02, width=0.02)\n",
    "    # cbar = fig.colorbar(q, cax=cax,label='inlier probability [-]')\n",
    "    ax.set_axis_off()\n",
    "    # ax.set_yticks([-7.55,-7.54,-7.53])\n",
    "    # ax.set_xticks([110.43, 110.44, 110.45])\n",
    "    # ax.set_xticklabels(['110.43','110.44','110.45'])\n",
    "    # ax.set_xlabel(r'Longitude [$^\\circ$E]')\n",
    "    # ax.set_ylabel(r'Latitude [$^\\circ$N]')\n",
    "    qk = ax.quiverkey(q,\n",
    "                             0.5,\n",
    "                             0.95,\n",
    "                             qk_length,\n",
    "                             str(qk_length) +' m displacement in slant rangeazimuth plane',\n",
    "                             labelpos = 'E',\n",
    "                             coordinates='figure')\n",
    "    # if attr != []:\n",
    "    #     ax.set_title(f'{attr}: min = {np.nanmax([np.nanmin(attr_copy),attr_lims[0]]):1.3f} max = {np.nanmin([np.nanmax(attr_copy),attr_lims[1]]):1.3f}')\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%matplotlib osx\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "alpha = 0.3\n",
    "q1 = plot_vec_attr_alpha(example_pairs[0].Stack[2],'Mag',5,50,0.01,[0,1],5,SHADING,DEM_EXTENT,[-7.565, -7.515],[110.415,110.468],alpha=alpha,fig_ax=(fig,ax))\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "sm.plot.plot_ra_offsets(example_pairs[0].Stack[2],SHADING,DEM_EXTENT,[-1,1],grid_size=1000,lat_lims=[-7.565, -7.515],lon_lims=[110.415,110.468],cmap=cm.vik)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib osx\n",
    "# import matplotlib as mpl\n",
    "# mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "# mpl.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig, ax = plt.subplots(3,5)\n",
    "alpha =0.3\n",
    "\n",
    "for i, obj in enumerate(example_pairs[0].Stack):\n",
    "\n",
    "    obj_cp = copy.deepcopy(obj)\n",
    "\n",
    "    # get attr.\n",
    "    mag = getattr(obj_cp,'Mag')\n",
    "    med_diff = getattr(obj_cp,'Mag_off_med_diff_7')\n",
    "    LOF_scores = getattr(obj_cp, f\"LOF_outlier_scores_300\")\n",
    "    HDBSCAN_labels = getattr(obj_cp, f\"HDBSCAN_probabilities_300_1\")\n",
    "    HDBSCAN_labels2 = getattr(obj_cp, f\"HDBSCAN_probabilities_300_30\")\n",
    "    GLOSH_labels = getattr(obj_cp, f\"HDBSCAN_outlier_scores_300_1\")\n",
    "\n",
    "    # calculate probablities\n",
    "    p_in_mag = 1 - (mag-np.nanmin(mag))/(np.nanmax(mag)-np.nanmin(mag))\n",
    "    p_in_med = 1 - med_diff/(np.nanmax(med_diff))\n",
    "    p_in_LOF = (LOF_scores-np.nanmin(LOF_scores))/(np.nanmax(LOF_scores)-np.nanmin(LOF_scores))\n",
    "    p_in_HDBSCAN = HDBSCAN_labels\n",
    "    p_in_HDBSCAN2 = HDBSCAN_labels2\n",
    "    p_in_GLOSH = 1 - GLOSH_labels\n",
    "    p_in_comb23 = (p_in_HDBSCAN+p_in_med)/2 \n",
    "\n",
    "    # assign as attributes\n",
    "    setattr(obj_cp,'p_in_mag',p_in_mag)\n",
    "    setattr(obj_cp,'p_in_med',p_in_med)\n",
    "    setattr(obj_cp,'p_in_LOF',p_in_LOF)\n",
    "    setattr(obj_cp,'p_in_HDBSCAN',p_in_HDBSCAN)\n",
    "    setattr(obj_cp,'p_in_HDBSCAN2',p_in_HDBSCAN2)\n",
    "    setattr(obj_cp,'p_in_GLOSH',p_in_GLOSH)\n",
    "    setattr(obj_cp,'p_in_comb23',p_in_comb23)\n",
    "\n",
    "    q1 = plot_vec_attr_alpha(obj_cp,'p_in_med',5,50,0.01,[0,1],5,SHADING,DEM_EXTENT,[-7.545, -7.535],[110.435,110.448],alpha=alpha,fig_ax=(fig,ax[0,i]))\n",
    "    q2 = plot_vec_attr_alpha(obj_cp,'p_in_HDBSCAN',5,50,0.01,[0,1],5,SHADING,DEM_EXTENT,[-7.545, -7.535],[110.435,110.448],alpha=alpha,fig_ax=(fig,ax[1,i]))\n",
    "    q3 = plot_vec_attr_alpha(obj_cp,'p_in_comb23',5,50,0.01,[0,1],5,SHADING,DEM_EXTENT,[-7.545, -7.535],[110.435,110.448],alpha=alpha,fig_ax=(fig,ax[2,i]))\n",
    "\n",
    "a=1\n",
    "cax = add_right_cax(ax[0,4], pad=0.02, width=0.02)\n",
    "cbar = fig.colorbar(q1, cax=cax,label='inlier probability [-]')\n",
    "cax = add_right_cax(ax[1,4], pad=0.02, width=0.02)\n",
    "cbar = fig.colorbar(q2, cax=cax,label='inlier probability [-]')\n",
    "cax = add_right_cax(ax[2,4], pad=0.02, width=0.02)\n",
    "cbar = fig.colorbar(q3, cax=cax,label='inlier probability [-]')\n",
    "plt.show()\n",
    "\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib osx\n",
    "# import matplotlib as mpl\n",
    "# mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "# mpl.rcParams.update({'font.size': 14})\n",
    "\n",
    "fig, ax = plt.subplots(2,3)\n",
    "alpha =0.3\n",
    "\n",
    "for i, (obj,ax_i) in enumerate(zip(example_pairs[0].Stack,ax.ravel())):\n",
    "\n",
    "    obj_cp = copy.deepcopy(obj)\n",
    "\n",
    "    # get attr.\n",
    "    med_diff = getattr(obj_cp,'Mag_off_med_diff_7')\n",
    "\n",
    "\n",
    "    # calculate probablities\n",
    "    p_in_med = 1 - med_diff/(np.nanmax(med_diff))\n",
    "\n",
    "    # assign as attributes\n",
    "    setattr(obj_cp,'p_in_med',p_in_med)\n",
    "\n",
    "\n",
    "    q1 = plot_vec_attr_alpha(obj_cp,'p_in_med',5,50,0.01,[0.9,1],5,SHADING,DEM_EXTENT,[-7.545, -7.535],[110.435,110.448],alpha=alpha,fig_ax=(fig,ax_i))\n",
    "\n",
    "a=1\n",
    "cax = add_right_cax(ax[1,1], pad=0.02, width=0.02)\n",
    "cbar = fig.colorbar(q1, cax=cax,label='inlier probability [-]')\n",
    "ax[1,2].set_axis_off()\n",
    "plt.show()\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lava flow files:\n",
    "L1888_FILE = '/Users/markbemelmans/Documents/PhD/projects/Merapi2021/merapi_maps/L1888_v2.shp'\n",
    "L1992_FILE = '/Users/markbemelmans/Documents/PhD/projects/Merapi2021/merapi_maps/L1992.shp'\n",
    "L1998_FILE = '/Users/markbemelmans/Documents/PhD/projects/Merapi2021/merapi_maps/L1998.shp'\n",
    "CRATER_FILE = '/Users/markbemelmans/Documents/PhD/projects/Merapi2021/merapi_maps/Merapi_crater.shp'\n",
    "\n",
    "# Read the shapefile\n",
    "L1888 = gpd.read_file(L1888_FILE)\n",
    "L1992 = gpd.read_file(L1992_FILE)\n",
    "L1998 = gpd.read_file(L1998_FILE)\n",
    "CRATER = gpd.read_file(CRATER_FILE)\n",
    "\n",
    "# Extract latitude and longitude into separate columns\n",
    "coords_L1888 = np.array(list(L1888[\"geometry\"][0].coords))\n",
    "coords_L1992 = np.array(list(L1992[\"geometry\"][0].coords))\n",
    "coords_L1998 = np.array(list(L1998[\"geometry\"][0].coords))\n",
    "coords_CRATER = np.array(list(CRATER[\"geometry\"][0].coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib osx\n",
    "lon_lims = [110.428, 110.45]\n",
    "lat_lims = [-7.550, -7.528]\n",
    "# import cmcrameri as cm\n",
    "\n",
    "def plot_ra_offsets(obj,shading,dem_extent,clims,grid_size=1000,lat_lims=[],lon_lims=[],cmap=cm.vik):\n",
    "    # get length of 1 deg. for scalebar\n",
    "    distance_meters = sm.plot.get_1deg_dist()\n",
    "        # change limits for nicer plotting\n",
    "    R_off = getattr(obj,'R_off')\n",
    "    A_off = getattr(obj,'A_off')\n",
    "    Lon_off = getattr(obj,'Lon_off')\n",
    "    Lat_off = getattr(obj,'Lat_off')\n",
    "    \n",
    "\n",
    "    if lat_lims == []:\n",
    "        lat_lims = [np.min(Lat_off),np.max(Lat_off)]\n",
    "    \n",
    "    if lon_lims == []:\n",
    "        lon_lims = [np.min(Lon_off),np.max(Lon_off)]\n",
    "\n",
    "    # plotting\n",
    "    fig1, axes = plt.subplots(2,1,figsize=(8,8))\n",
    "    axes[0].imshow(shading,cmap=cm.grayC,alpha=0.5, extent=dem_extent)\n",
    "    axes[1].imshow(shading,cmap=cm.grayC,alpha=0.5, extent=dem_extent)\n",
    "\n",
    "    plot_data = axes[0].hexbin(Lon_off.flatten(),Lat_off.flatten(),C=R_off.flatten(),gridsize=grid_size,cmap=cmap,vmin=clims[0],vmax=clims[1])\n",
    "    axes[0].set_xlim(lon_lims)\n",
    "    axes[0].set_ylim(lat_lims)\n",
    "    axes[0].set_aspect('equal', 'box')\n",
    "    axes[0].add_artist(ScaleBar(distance_meters,location='lower left'))\n",
    "    # axes[0].set_axis_off()\n",
    "    cbar = plt.colorbar(plot_data,ax=axes[0])\n",
    "    cbar.set_label('Slant range offset [m]')\n",
    "\n",
    "    plot_data = axes[1].hexbin(Lon_off.flatten(),Lat_off.flatten(),C=A_off.flatten(),gridsize=grid_size,cmap=cmap,vmin=clims[0],vmax=clims[1])\n",
    "    axes[1].set_xlim(lon_lims)\n",
    "    axes[1].set_ylim(lat_lims)\n",
    "    axes[1].set_aspect('equal', 'box')\n",
    "    axes[1].add_artist(ScaleBar(distance_meters,location='lower left'))\n",
    "    # axes[0].set_axis_off()\n",
    "    cbar = plt.colorbar(plot_data,ax=axes[1])\n",
    "    cbar.set_label('Azimuth offset [m]')\n",
    "    return fig1, axes\n",
    "\n",
    "fig, ax = plot_ra_offsets(example_pairs[0].Stack[0],SHADING,DEM_EXTENT,[-3,3],500,lat_lims=lat_lims,lon_lims=lon_lims,cmap=cm.vik)\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "ax[0].plot(coords_L1888[:,0],coords_L1888[:,1],linewidth=2,color='tab:blue',label='L1888')\n",
    "ax[0].plot(coords_L1992[:,0],coords_L1992[:,1],linewidth=2,color='tab:pink',label='L1992')\n",
    "ax[0].plot(coords_L1998[:,0],coords_L1998[:,1],linewidth=2,color='tab:green',label='L1998')\n",
    "ax[0].plot(coords_CRATER[:,0],coords_CRATER[:,1],linewidth=2,color='black',linestyle='--',label='Crater rim')\n",
    "ax[0].legend(loc='upper left')\n",
    "ax[1].plot(coords_L1888[:,0],coords_L1888[:,1],linewidth=2,color='tab:blue')\n",
    "ax[1].plot(coords_L1992[:,0],coords_L1992[:,1],linewidth=2,color='tab:pink')\n",
    "ax[1].plot(coords_L1998[:,0],coords_L1998[:,1],linewidth=2,color='tab:green')\n",
    "ax[1].plot(coords_CRATER[:,0],coords_CRATER[:,1],linewidth=2,color='black',linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sequential median (0.9) then \n",
    "\n",
    "cut_off_med = 0.9\n",
    "\n",
    "\n",
    "for obj in example_pairs[0].Stack:\n",
    "    print(obj)\n",
    "    obj.rem_outliers_median(7,cut_off_med)\n",
    "    obj.reset_vecs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot outlier removed maps\n",
    "lon_lims = [110.428, 110.45]\n",
    "lat_lims = [-7.550, -7.528]\n",
    "fig, ax = plot_ra_offsets(example_pairs[0].Stack[4],SHADING,DEM_EXTENT,[-3,3],500,lat_lims=lat_lims,lon_lims=lon_lims,cmap=cm.vik)\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "ax[0].plot(coords_L1888[:,0],coords_L1888[:,1],linewidth=2,color='tab:blue',label='L1888')\n",
    "ax[0].plot(coords_L1992[:,0],coords_L1992[:,1],linewidth=2,color='tab:pink',label='L1992')\n",
    "ax[0].plot(coords_L1998[:,0],coords_L1998[:,1],linewidth=2,color='tab:green',label='L1998')\n",
    "ax[0].plot(coords_CRATER[:,0],coords_CRATER[:,1],linewidth=2,color='black',linestyle='--',label='Crater rim')\n",
    "ax[1].plot(coords_L1888[:,0],coords_L1888[:,1],linewidth=2,color='tab:blue')\n",
    "ax[1].plot(coords_L1992[:,0],coords_L1992[:,1],linewidth=2,color='tab:pink')\n",
    "ax[1].plot(coords_L1998[:,0],coords_L1998[:,1],linewidth=2,color='tab:green')\n",
    "ax[1].plot(coords_CRATER[:,0],coords_CRATER[:,1],linewidth=2,color='black',linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_MKA(q_obj,indeces=[],window_size=1,comp_lim=0.5,CI_lim=5):\n",
    "        # \"\"\"\n",
    "        # Run Multi-kernel averaging where user can define seleced indices from the stack, \n",
    "        # desired window size, and a completion factor as a high pass filter\n",
    "\n",
    "        # Args:\n",
    "        #     indices (list, optional): indices of slices from datastack used for MKA. Defaults to [], use all data.\n",
    "        #     window_size (int, optional): window dimension for MKA, odd numbers \n",
    "        #                                  prefered because of pixel centering. \n",
    "        #                                  Defaults to 1.\n",
    "        #     comp_lim (float, optional): completion limit between [0.0, 1.0]\n",
    "        #                                 Only take data for MKA if more than \n",
    "        #                                 comp_lim of the stack is not nan. \n",
    "        #                                 Defaults to 0.5.\n",
    "\n",
    "        # Returns:\n",
    "        #     avg_map: Multi-kernel Average map\n",
    "        # \"\"\"\n",
    "        # get stack data\n",
    "        if indeces==[]:\n",
    "            stack_R = [obj.R_off for obj in q_obj.Stack]\n",
    "            stack_A = [obj.A_off for obj in q_obj.Stack]\n",
    "            stack_ccp = [obj.Ccp_off for obj in q_obj.Stack]\n",
    "            stack_ccs = [obj.Ccs_off for obj in q_obj.Stack]\n",
    "        else:\n",
    "            substack = [q_obj.Stack[i] for i in indeces]\n",
    "            stack_R = np.stack([obj.R_off for obj in substack],axis=0)\n",
    "            stack_A = np.stack([obj.A_off for obj in substack],axis=0)\n",
    "            stack_ccp = np.stack([obj.Ccp_off for obj in substack],axis=0)\n",
    "            stack_ccs = np.stack([obj.Ccs_off for obj in substack],axis=0)\n",
    "\n",
    "        # create list of maps to make \n",
    "        avg_maps = []\n",
    "\n",
    "        for stack in [stack_R,stack_A]:\n",
    "            # set window size according to stack dimensions\n",
    "            window_shape = (stack.shape[0], window_size, window_size)\n",
    "            print(np.shape(window_shape))\n",
    "            # use np.lib.stride_tricks.sliding_window_view to devided data into windows\n",
    "            # 1.2xfaster than sklearn view_as_windows\n",
    "            win_data = np.lib.stride_tricks.sliding_window_view(stack, window_shape)[0]\n",
    "\n",
    "            # remove data that is nan for too many different window sizes\n",
    "            nan_frac = np.sum(np.isnan(win_data), axis=2) / (window_size ** 2)\n",
    "            nan_frac = nan_frac/np.shape(win_data)[2]\n",
    "            nan_frac[nan_frac > comp_lim] = np.nan\n",
    "            nan_frac[nan_frac <= comp_lim] = 1\n",
    "            win_data = np.multiply(win_data, nan_frac[..., np.newaxis])\n",
    "\n",
    "            # define shape of multi-kernel averaged map (same as input data), filled with nan\n",
    "            Avg_map = np.full(stack.shape[1:], np.nan)\n",
    "\n",
    "            # per window, go take 95% confidence interval data and take average (mean)\n",
    "            for win_i in range(win_data.shape[0]):\n",
    "                if win_i % 50 == 0:\n",
    "                    print('win_i', win_i)\n",
    "                for win_j in range(win_data.shape[1]):\n",
    "                    offset = window_size // 2\n",
    "                    # extract relevant window\n",
    "                    win = win_data[win_i, win_j]\n",
    "                    # calculate 95 % confidence interval\n",
    "                    if np.size(win)==1:\n",
    "                         Avg_map[win_i + offset, win_j + offset] = np.nanmean(win)\n",
    "                    elif CI_lim == 0:\n",
    "                         Avg_map[win_i + offset, win_j + offset] = np.nanmean(win)\n",
    "                    else:\n",
    "                        percentiles = np.nanpercentile(win, [CI_lim/2, 100-(CI_lim/2)])\n",
    "                        # mask data outside 95% confidence interval with nan\n",
    "                        mask = (win < percentiles[0]) | (win > percentiles[1])\n",
    "                        win[mask] = np.nan\n",
    "                        # calculate mean of window (offset by floor(window_size/2) because of border)\n",
    "                        \n",
    "                        Avg_map[win_i + offset, win_j + offset] = np.nanmean(win)\n",
    "            avg_maps.append(Avg_map)\n",
    "        q_obj.MKA_R_off = avg_maps[0]\n",
    "        q_obj.MKA_A_off = avg_maps[1]\n",
    "        # stack_obj.MKA_Ccp_off = avg_maps[2]\n",
    "        # stack_obj.MKA_Ccs_off = avg_maps[3]\n",
    "\n",
    "        return q_obj.MKA_R_off, q_obj.MKA_A_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeces = [0,1,2,3,4]\n",
    "MKA_R_off, MKA_A_off = Run_MKA(example_pairs[0],indeces,1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MKA reuslt\n",
    "# query points\n",
    "q1 = [110.446216,-7.536389] # stable no veg north\n",
    "q2 = [110.42647,-7.53297] # stable veg north west\n",
    "q3 = [110.4421429,-7.5377992] # L1888\n",
    "q4 = [110.44214,-7.54237] # L1998 large disp\n",
    "q5 = [110.44013,-7.54678] # stable no veg south\n",
    "q6 = [110.44200,-7.53604] # L1956 offset gradient\n",
    "query_points = np.stack((q1,q2,q3,q4,q5,q6))\n",
    "r = 50\n",
    "\n",
    "[q_mean_MKA, q_median_MKA, q_std_MKA, q_95_MKA], coordinate_circles = example_pairs[0].query_point_MKA('MKA_R_off',\n",
    "                                                                                                 query_points[:,1],\n",
    "                                                                                                 query_points[:,0],\n",
    "                                                                                                 r)  \n",
    "print('\\n')\n",
    "print(f'Range mean for A, B, C: {q_mean_MKA}')\n",
    "print(f'Range median for A, B, C: {q_median_MKA}')\n",
    "print(f'Range standard deviation for A, B, C: {q_std_MKA}')\n",
    "print(f'Range 95\\% confidence interval fro A, B, C: {q_95_MKA}')\n",
    "\n",
    "[q_mean_MKA, q_median_MKA, q_std_MKA, q_95_MKA], coordinate_circles = example_pairs[0].query_point_MKA('MKA_A_off',\n",
    "                                                                                                 query_points[:,1],\n",
    "                                                                                                 query_points[:,0],\n",
    "                                                                                                 r)  \n",
    "print('\\n')\n",
    "print(f'Azimuth mean for A, B, C: {q_mean_MKA}')\n",
    "print(f'Azimuth median for A, B, C: {q_median_MKA}')\n",
    "print(f'Azimuth standard deviation for A, B, C: {q_std_MKA}')\n",
    "print(f'Azimuth 95\\% confidence interval fro A, B, C: {q_95_MKA}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from cmcrameri import cm\n",
    "\n",
    "import pyproj\n",
    "from pyproj import CRS\n",
    "def plot_ra_offsets_MKA(obj,MKA_R_off,MKA_A_off,shading,dem_extent,clims,grid_size=1000,lat_lims=[],lon_lims=[],cmap=cm.vik,width=[]):\n",
    "    # get length of 1 deg. for scalebar\n",
    "    distance_meters = sm.plot.get_1deg_dist()\n",
    "        # change limits for nicer plotting\n",
    "    if width==[]:\n",
    "        Lon_off = getattr(obj.Stack[0],'Lon_off')\n",
    "        Lat_off = getattr(obj.Stack[0],'Lat_off')\n",
    "    else:\n",
    "        obj.get_latlon_from_file(width)\n",
    "        obj.add_lat_lon_to_data(R_START,A_START)\n",
    "        obj.crop_stack_ccs(R_STEP,A_STEP)\n",
    "    \n",
    "\n",
    "    rng = np.linspace(obj.Limits[0],obj.Limits[1],int((obj.Limits[1]-obj.Limits[0])/R_STEP)+1)\n",
    "    azi = np.linspace(obj.Limits[2],obj.Limits[3],int((obj.Limits[3]-obj.Limits[2])/A_STEP)+1)\n",
    "    RNG, AZI = np.meshgrid(rng,azi)\n",
    "\n",
    "\n",
    "    # find number of range estimates\n",
    "    for d in obj.Mask_data:\n",
    "        Lat_off = np.full(np.shape(RNG)[::-1], np.nan)\n",
    "        Lon_off = np.full(np.shape(RNG)[::-1], np.nan)\n",
    "\n",
    "        # fill arrays from indexes \n",
    "        Lat_off[d[11],d[12]] = d[9]\n",
    "        Lon_off[d[11],d[12]] = d[10]\n",
    "\n",
    "    if lat_lims == []:\n",
    "        lat_lims = [np.min(Lat_off),np.max(Lat_off)]\n",
    "    \n",
    "    if lon_lims == []:\n",
    "        lon_lims = [np.min(Lon_off),np.max(Lon_off)]\n",
    "\n",
    "    # plotting\n",
    "    fig1, axes = plt.subplots(2,1,figsize=(8,8))\n",
    "    axes[0].imshow(shading,cmap=cm.grayC,alpha=0.5, extent=dem_extent)\n",
    "    axes[1].imshow(shading,cmap=cm.grayC,alpha=0.5, extent=dem_extent)\n",
    "\n",
    "    plot_data = axes[0].hexbin(Lon_off.flatten(),Lat_off.flatten(),C=MKA_R_off.flatten(),gridsize=grid_size,cmap=cmap,vmin=clims[0],vmax=clims[1])\n",
    "    axes[0].set_xlim(lon_lims)\n",
    "    axes[0].set_ylim(lat_lims)\n",
    "    axes[0].set_aspect('equal', 'box')\n",
    "    axes[0].add_artist(ScaleBar(distance_meters,location='lower left'))\n",
    "    # axes[0].set_axis_off()\n",
    "    cbar = plt.colorbar(plot_data,ax=axes[0])\n",
    "    cbar.set_label('Slant range offset [m]')\n",
    "\n",
    "    plot_data = axes[1].hexbin(Lon_off.flatten(),Lat_off.flatten(),C=MKA_A_off.flatten(),gridsize=grid_size,cmap=cmap,vmin=clims[0],vmax=clims[1])\n",
    "    axes[1].set_xlim(lon_lims)\n",
    "    axes[1].set_ylim(lat_lims)\n",
    "    axes[1].set_aspect('equal', 'box')\n",
    "    axes[1].add_artist(ScaleBar(distance_meters,location='lower left'))\n",
    "    # axes[0].set_axis_off()\n",
    "    cbar = plt.colorbar(plot_data,ax=axes[1])\n",
    "    cbar.set_label('Azimuth offset [m]')\n",
    "    return fig1, axes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MKA vec data\n",
    "\n",
    "def get_MKA_vec(obj):\n",
    "    Lon_off = getattr(obj.Stack[0],'Lon_off')\n",
    "    Lat_off = getattr(obj.Stack[0],'Lat_off')\n",
    "\n",
    "    rng = np.linspace(obj.Limits[0],obj.Limits[1],int((obj.Limits[1]-obj.Limits[0])/R_STEP)+1)\n",
    "    azi = np.linspace(obj.Limits[2],obj.Limits[3],int((obj.Limits[3]-obj.Limits[2])/A_STEP)+1)\n",
    "    RNG, AZI = np.meshgrid(rng,azi)\n",
    "\n",
    "    # find number of range estimates\n",
    "    for d in obj.Mask_data:\n",
    "        Lat_off = np.full(np.shape(RNG)[::-1], np.nan)\n",
    "        Lon_off = np.full(np.shape(RNG)[::-1], np.nan)\n",
    "\n",
    "        # fill arrays form indexes \n",
    "        Lat_off[d[11],d[12]] = d[9]\n",
    "        Lon_off[d[11],d[12]] = d[10]\n",
    "    setattr(obj,'Lon_off_MKA',Lon_off)\n",
    "    setattr(obj,'Lat_off_MKA',Lat_off)\n",
    "    \n",
    "    nan_mask = np.isnan(obj.MKA_R_off)\n",
    "    MKA_R_off_vec = obj.MKA_R_off[~nan_mask].ravel()\n",
    "    MKA_A_off_vec = obj.MKA_A_off[~nan_mask].ravel()\n",
    "    Lon_off_MKA_vec = obj.Lon_off_MKA[~nan_mask].ravel()\n",
    "    Lat_off_MKA_vec = obj.Lat_off_MKA[~nan_mask].ravel()\n",
    "\n",
    "    # set attributes\n",
    "    setattr(obj,'MKA_R_off_vec', MKA_R_off_vec)\n",
    "    setattr(obj,'MKA_A_off_vec', MKA_A_off_vec)\n",
    "    setattr(obj,'Lon_off_MKA_vec', Lon_off_MKA_vec)\n",
    "    setattr(obj,'Lat_off_MKA_vec', Lat_off_MKA_vec)\n",
    "\n",
    "    return MKA_R_off_vec, MKA_A_off_vec, Lon_off_MKA_vec, Lat_off_MKA_vec\n",
    "\n",
    "# get_MKA_vec(example_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib osx\n",
    "## plot MKA results\n",
    "lon_lims = [110.428, 110.45]\n",
    "lat_lims = [-7.550, -7.528]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plot_ra_offsets_MKA(example_pairs[0],MKA_R_off,MKA_A_off,SHADING,DEM_EXTENT,[-3,3],500,lat_lims=lat_lims,lon_lims=lon_lims,cmap=cm.vik,width=WIDTH)\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "ax[0].plot(coords_L1888[:,0],coords_L1888[:,1],linewidth=2,color='tab:blue',label='L1888')\n",
    "ax[0].plot(coords_L1992[:,0],coords_L1992[:,1],linewidth=2,color='tab:pink',label='L1992')\n",
    "ax[0].plot(coords_L1998[:,0],coords_L1998[:,1],linewidth=2,color='tab:green',label='L1998')\n",
    "ax[0].plot(coords_CRATER[:,0],coords_CRATER[:,1],linewidth=2,color='black',linestyle='--',label='Crater rim')\n",
    "ax[1].plot(coords_L1888[:,0],coords_L1888[:,1],linewidth=2,color='tab:blue')\n",
    "ax[1].plot(coords_L1992[:,0],coords_L1992[:,1],linewidth=2,color='tab:pink')\n",
    "ax[1].plot(coords_L1998[:,0],coords_L1998[:,1],linewidth=2,color='tab:green')\n",
    "ax[1].plot(coords_CRATER[:,0],coords_CRATER[:,1],linewidth=2,color='black',linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib osx\n",
    "# swath profile simple: \n",
    "\n",
    "from functools import partial\n",
    "from mpl_point_clicker import clicker # for user input from map\n",
    "from inpoly import inpoly2 # for fast inpolygon checks\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from shapely.geometry import Polygon, LineString\n",
    "# from mpl_interactions import zoom_factory, panhandler\n",
    "\n",
    "\n",
    "# define variables:\n",
    "\n",
    "grid_size = 1000\n",
    "cmap = cm.vik\n",
    "clims= [-3,3]\n",
    "lon_lims = [110.428, 110.45]\n",
    "lat_lims = [-7.550, -7.528]\n",
    "# create swath profile from user input\n",
    "\n",
    "\n",
    "def poly_line_buffer_lat_lon(line_coords,width):\n",
    "    line = LineString(line_coords)\n",
    "    local_azimuthal_projection = f\"+proj=aeqd +R=6371000 +units=m +lat_0={line.coords[0][1]} +lon_0={line.coords[0][0]}\"\n",
    "\n",
    "    wgs84_to_aeqd = partial(\n",
    "            pyproj.transform,\n",
    "            pyproj.Proj('+proj=longlat +datum=WGS84 +no_defs'),\n",
    "            pyproj.Proj(local_azimuthal_projection),\n",
    "            )\n",
    "\n",
    "    aeqd_to_wgs84 = partial(\n",
    "            pyproj.transform,\n",
    "            pyproj.Proj(local_azimuthal_projection),\n",
    "            pyproj.Proj('+proj=longlat +datum=WGS84 +no_defs'),\n",
    "            )\n",
    "    line_transformed = transform(wgs84_to_aeqd, line)\n",
    "    buffer_line = line_transformed.buffer(width/2,cap_style=2)# capstyle 2 for flat caps the meet profile (so do not go beyond the line)\n",
    "    return transform(aeqd_to_wgs84, buffer_line)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax0 =plt.subplots(constrained_layout=True, figsize=(15,10))\n",
    "plot_data = ax0.hexbin(example_pairs[0].Stack[0].Lon_off.flatten(),example_pairs[0].Stack[0].Lat_off.flatten(),C=MKA_R_off.flatten(),gridsize=grid_size,cmap=cmap,vmin=clims[0],vmax=clims[1])\n",
    "ax0.set_xlim(lon_lims)\n",
    "ax0.set_ylim(lat_lims)\n",
    "ax0.set_aspect('equal', 'box')\n",
    "ax0.add_artist(ScaleBar(110123.8348,location='lower left'))\n",
    "ax0.plot(coords_L1888[:,0],coords_L1888[:,1],linewidth=2,color='tab:blue',label='L1888')\n",
    "ax0.plot(coords_L1992[:,0],coords_L1992[:,1],linewidth=2,color='tab:pink',label='L1992')\n",
    "ax0.plot(coords_L1998[:,0],coords_L1998[:,1],linewidth=2,color='tab:green',label='L1998')\n",
    "ax0.plot(coords_CRATER[:,0],coords_CRATER[:,1],linewidth=2,color='black',linestyle='--',label='Crater rim')\n",
    "ax0.set_axis_off()\n",
    "# axes[0].set_axis_off()\n",
    "cbar = plt.colorbar(plot_data,ax=ax0)\n",
    "cbar.set_label('Slant range offset [m]')\n",
    "klicker = clicker(ax0, [\"query_point\"], markers=[\"x\"], colors='green',markersize=15,markeredgewidth=3,**{\"linestyle\": \"-\"})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import transform\n",
    "\n",
    "width_swath = 50 # m\n",
    "mouse_pos = klicker.get_positions()['query_point']\n",
    "start_base = mouse_pos[-2,:] # starting point for profile baseline\n",
    "end_base = mouse_pos[-1,:] # end point for profile baseline\n",
    "print('query line: ', start_base, end_base, '\\n width: ' , width_swath, ' m')\n",
    "\n",
    "# start_base = [110.4405519,-7.5337943] # starting point for profile baseline\n",
    "# end_base = [110.4442428,-7.5371901] # end point for profile baseline\n",
    "\n",
    "# get line coords and add buffer\n",
    "line_coords = np.asarray([start_base,end_base])\n",
    "buffer_poly = poly_line_buffer_lat_lon(line_coords,width_swath)\n",
    "\n",
    "\n",
    "# show line with buffer\n",
    "fig, ax0 =plt.subplots(constrained_layout=True, figsize=(15,10))\n",
    "plot_data = ax0.hexbin(example_pairs[0].Stack[0].Lon_off.flatten(),example_pairs[0].Stack[0].Lat_off.flatten(),C=MKA_R_off.flatten(),gridsize=grid_size,cmap=cmap,vmin=clims[0],vmax=clims[1])\n",
    "ax0.set_xlim(lon_lims)\n",
    "ax0.set_ylim(lat_lims)\n",
    "ax0.set_aspect('equal', 'box')\n",
    "ax0.add_artist(ScaleBar(110123.8348,location='lower left'))\n",
    "# axes[0].set_axis_off()\n",
    "cbar = plt.colorbar(plot_data,ax=ax0)\n",
    "cbar.set_label('Range offset [m]')\n",
    "ax0.plot(np.asarray(buffer_poly.exterior.coords)[:,0],np.asarray(buffer_poly.exterior.coords)[:,1])\n",
    "ax0.plot(np.asarray(line_coords)[:,0],np.asarray(line_coords[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import shapely\n",
    "import warnings\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "SHAPELY_GE_20 = str(shapely.__version__) >= LooseVersion(\"2.0\")\n",
    "\n",
    "try:\n",
    "    from shapely.errors import ShapelyDeprecationWarning as shapely_warning\n",
    "except ImportError:\n",
    "    shapely_warning = None\n",
    "\n",
    "if shapely_warning is not None and not SHAPELY_GE_20:\n",
    "    @contextlib.contextmanager\n",
    "    def ignore_shapely2_warnings():\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=shapely_warning)\n",
    "            yield\n",
    "else:\n",
    "    @contextlib.contextmanager\n",
    "    def ignore_shapely2_warnings():\n",
    "        yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all points within polygon\n",
    "from skspatial.objects import Line as sksline\n",
    "import scipy.spatial.transform \n",
    "line = sksline.from_points(point_a=start_base, point_b=end_base)\n",
    "\n",
    "def geodetic2enu(lat, lon, alt, lat_org, lon_org, alt_org):\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        {\"proj\":'latlong', \"ellps\":'WGS84', \"datum\":'WGS84'},\n",
    "        {\"proj\":'geocent', \"ellps\":'WGS84', \"datum\":'WGS84'},\n",
    "        )\n",
    "    x, y, z = transformer.transform( lon,lat,  alt,radians=False)\n",
    "    x_org, y_org, z_org = transformer.transform( lon_org,lat_org,  alt_org,radians=False)\n",
    "    vec=np.array([[ x-x_org, y-y_org, z-z_org]]).T\n",
    "    \n",
    "    rot1 =  scipy.spatial.transform.Rotation.from_euler('x', -(90-lat_org), degrees=True).as_matrix()#angle*-1 : left handed *-1\n",
    "    rot3 =  scipy.spatial.transform.Rotation.from_euler('z', -(90+lon_org), degrees=True).as_matrix()#angle*-1 : left handed *-1\n",
    "    rotMatrix = rot1.dot(rot3)  \n",
    "    \n",
    "    enu = np.squeeze(rotMatrix.dot(vec).T)\n",
    "    return enu\n",
    "\n",
    "\n",
    "isin_list = []\n",
    "proj_point_list = []\n",
    "obj_list = [obj for obj in example_pairs[0].Stack]\n",
    "obj_list.append(MKA_data_list[0])\n",
    "obj_list.append(MKA_data_list[1])\n",
    "print(obj_list)\n",
    "lon_off_vec_list = ['Lon_off_vec','Lon_off_vec','Lon_off_vec','Lon_off_vec','Lon_off_vec','Lon_off_MKA_vec','Lon_off_MKA_vec']\n",
    "lat_off_vec_list = ['Lat_off_vec','Lat_off_vec','Lat_off_vec','Lat_off_vec','Lat_off_vec','Lat_off_MKA_vec','Lat_off_MKA_vec']\n",
    "R_off_vec_list = ['R_off_vec','R_off_vec','R_off_vec','R_off_vec','R_off_vec','MKA_R_off_vec','MKA_R_off_vec']\n",
    "A_off_vec_list = ['A_off_vec','A_off_vec','A_off_vec','A_off_vec','A_off_vec','MKA_A_off_vec','MKA_A_off_vec']\n",
    "\n",
    "\n",
    "for (obj,lon_off_name,lat_off_name,R_off_name) in zip(obj_list,lon_off_vec_list,lat_off_vec_list,R_off_vec_list):\n",
    "    lons = getattr(obj,lon_off_name)\n",
    "    lats = getattr(obj,lat_off_name)\n",
    "    lon_lat = np.dstack((lons,lats))\n",
    "    isin, ison = inpoly2(np.squeeze(lon_lat),np.asarray(buffer_poly.exterior.coords))\n",
    "    print(np.count_nonzero(isin))\n",
    "    isin_list.append(isin)\n",
    "    proj_point = np.empty(shape=(np.count_nonzero(isin), 3), dtype=float) # pre-alloc nd-array                                  0)\n",
    "\n",
    "    with ignore_shapely2_warnings():\n",
    "        for p_id, lon_lat_id in enumerate(zip(np.squeeze(lon_lat)[isin,:],getattr(obj,R_off_name)[isin])):\n",
    "            point = lon_lat_id[0]\n",
    "            if np.mod(p_id,1000)==0:\n",
    "                    print('fraction complete:',p_id/np.count_nonzero(isin)) # progress tracker\n",
    "            proj_point[p_id,:]=np.append(np.asarray(line.project_point(point)),lon_lat_id[1])\n",
    "    \n",
    "\n",
    "    # transform projected points to east,north,(up)\n",
    "    point_transformed = geodetic2enu(proj_point[:,1],\n",
    "                                    proj_point[:,0],\n",
    "                                    np.squeeze(np.zeros((np.count_nonzero(isin),1))).T,\n",
    "                                    start_base[1],\n",
    "                                    start_base[0],\n",
    "                                    0)\n",
    "    #lon,lat,dist along profile, disp\n",
    "    proj_point = np.stack((proj_point[:,0], \n",
    "                                proj_point[:,1],\n",
    "                                np.linalg.norm(point_transformed[:,0:2],axis=1),\n",
    "                                proj_point[:,2])).T\n",
    "    print(proj_point)\n",
    "    proj_point = proj_point[~np.isnan(proj_point[:,3])]\n",
    "    proj_point_list.append(proj_point)\n",
    "    \n",
    "# proj_points=line.project_point(point(np.squeeze(lon_lat)[isin,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proj_point_list[2])\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "ax.scatter(proj_point_list[2][:,2],proj_point_list[2][:,3])\n",
    "# ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textsize = 15\n",
    "plt.rc('font', size=textsize) \n",
    "# %matplotlib inline\n",
    "# bin profile output in sections with width xd to calculate percentiles\n",
    "dx = 10\n",
    "# offset = 0.2\n",
    "grid_size = 1000\n",
    "clims = [-3, 3]\n",
    "lon_lims = [110.436, 110.445]\n",
    "lat_lims = [-7.540, -7.534]\n",
    "\n",
    "colours = ['tab:blue','tab:orange','tab:green','tab:red','tab:purple','black', 'darkgrey']\n",
    "labels = [f'r:58, a:28', f'r:140, a:68 (+0.2 m)', f'r:224, a: 108 (+0.4 m)', f'r:306, a:148 (+0.6 m)', f'r:388, a: 188 (+0.8 m)', f'MKA 1-5 (+2 m)', f'MKA 1-3 (+2.2 m)']\n",
    "offsets = [0, 0.2, 0.4, 0.6, 0.8, 2, 2.2]\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "gs = GridSpec(4, 4, figure=fig)\n",
    "ax00 = fig.add_subplot(gs[0, 0]) # win 1\n",
    "ax01 = fig.add_subplot(gs[0, 1]) # win 2\n",
    "ax02 = fig.add_subplot(gs[0, 2]) # win 3\n",
    "ax10 = fig.add_subplot(gs[1, 0]) # win 4\n",
    "ax11 = fig.add_subplot(gs[1, 1]) # win 5\n",
    "\n",
    "ax03 = fig.add_subplot(gs[0, 3]) # mka 1-5\n",
    "ax13 = fig.add_subplot(gs[1, 3]) # mka 1-3\n",
    "ax2  = fig.add_subplot(gs[2:, :]) # swath profile\n",
    "\n",
    "\n",
    "for (ax, obj,colour) in zip([ax00,ax01,ax02,ax10,ax11],example_pairs[0].Stack,colours):\n",
    "    plot_data = ax.hexbin(obj.Lon_off.flatten(),obj.Lat_off.flatten(),C=obj.R_off.flatten(),gridsize=grid_size,cmap=cmap,vmin=clims[0],vmax=clims[1])\n",
    "    ax.set_xlim(lon_lims)\n",
    "    ax.set_ylim(lat_lims)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.add_artist(ScaleBar(110123.8348,location='lower left'))\n",
    "    # axes[0].set_axis_off()\n",
    "    # cbar = plt.colorbar(plot_data,ax=ax)\n",
    "    # cbar.set_label('Range offset [m]')\n",
    "    ax.plot(np.asarray(buffer_poly.exterior.coords)[:,0],np.asarray(buffer_poly.exterior.coords)[:,1],color='black')\n",
    "    ax.plot(np.asarray(line_coords)[:,0],np.asarray(line_coords[:,1]),color='red')\n",
    "    # ax.set_axis_off()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.plot(coords_L1888[:,0],coords_L1888[:,1],linewidth=2,color='tab:blue',label='L1888')\n",
    "    ax.plot(coords_L1992[:,0],coords_L1992[:,1],linewidth=2,color='tab:pink',label='L1992')\n",
    "    ax.plot(coords_L1998[:,0],coords_L1998[:,1],linewidth=2,color='tab:green',label='L1998')\n",
    "    ax.plot(coords_CRATER[:,0],coords_CRATER[:,1],linewidth=2,color='black',linestyle='--',label='Crater rim')\n",
    "    ax.spines['bottom'].set_color(colour)\n",
    "    ax.spines['top'].set_color(colour)\n",
    "    ax.spines['left'].set_color(colour)\n",
    "    ax.spines['right'].set_color(colour)\n",
    "    [ax.spines[i].set_linewidth(2) for i in ax.spines]\n",
    "\n",
    "ax03.hexbin(MKA_data_list[0].Lon_off_MKA_vec,MKA_data_list[0].Lat_off_MKA_vec,C=MKA_data_list[0].MKA_R_off_vec,gridsize=grid_size,cmap=cmap,vmin=clims[0],vmax=clims[1])\n",
    "ax03.set_xlim(lon_lims)\n",
    "ax03.set_ylim(lat_lims)\n",
    "ax03.set_aspect('equal', 'box')\n",
    "ax03.add_artist(ScaleBar(110123.8348,location='lower left'))\n",
    "\n",
    "# cbar = plt.colorbar(plot_data,ax=ax03)\n",
    "# cbar.set_label('Range offset [m]')\n",
    "ax03.plot(np.asarray(buffer_poly.exterior.coords)[:,0],np.asarray(buffer_poly.exterior.coords)[:,1],color='black')\n",
    "ax03.plot(np.asarray(line_coords)[:,0],np.asarray(line_coords[:,1]),color='red')\n",
    "ax03.plot(coords_L1888[:,0],coords_L1888[:,1],linewidth=2,color='tab:blue',label='L1888')\n",
    "ax03.plot(coords_L1992[:,0],coords_L1992[:,1],linewidth=2,color='tab:pink',label='L1992')\n",
    "ax03.plot(coords_L1998[:,0],coords_L1998[:,1],linewidth=2,color='tab:green',label='L1998')\n",
    "ax03.plot(coords_CRATER[:,0],coords_CRATER[:,1],linewidth=2,color='black',linestyle='--',label='Crater rim')\n",
    "ax03.spines['bottom'].set_color('black')\n",
    "ax03.spines['top'].set_color('black')\n",
    "ax03.spines['left'].set_color('black')\n",
    "ax03.spines['right'].set_color('black')\n",
    "[ax03.spines[i].set_linewidth(2) for i in ax03.spines]\n",
    "ax03.get_xaxis().set_visible(False)\n",
    "ax03.get_yaxis().set_visible(False)\n",
    "\n",
    "ax13.hexbin(MKA_data_list[1].Lon_off_MKA_vec,MKA_data_list[1].Lat_off_MKA_vec,C=MKA_data_list[1].MKA_R_off_vec,gridsize=grid_size,cmap=cmap,vmin=clims[0],vmax=clims[1])\n",
    "ax13.set_xlim(lon_lims)\n",
    "ax13.set_ylim(lat_lims)\n",
    "ax13.set_aspect('equal', 'box')\n",
    "ax13.add_artist(ScaleBar(110123.8348,location='lower left'))\n",
    "# add_right_cax(ax13,0.05,0.02)\n",
    "# cbar = plt.colorbar(plot_data,ax=ax13)\n",
    "# cbar.set_label('Range offset [m]')\n",
    "ax13.plot(np.asarray(buffer_poly.exterior.coords)[:,0],np.asarray(buffer_poly.exterior.coords)[:,1],color='black')\n",
    "ax13.plot(np.asarray(line_coords)[:,0],np.asarray(line_coords[:,1]),color='red')\n",
    "\n",
    "ax13.plot(coords_L1888[:,0],coords_L1888[:,1],linewidth=2,color='tab:blue',label='L1888')\n",
    "ax13.plot(coords_L1992[:,0],coords_L1992[:,1],linewidth=2,color='tab:pink',label='L1992')\n",
    "ax13.plot(coords_L1998[:,0],coords_L1998[:,1],linewidth=2,color='tab:green',label='L1998')\n",
    "ax13.plot(coords_CRATER[:,0],coords_CRATER[:,1],linewidth=2,color='black',linestyle='--',label='Crater rim')\n",
    "ax13.spines['bottom'].set_color('darkgrey')\n",
    "ax13.spines['top'].set_color('darkgrey')\n",
    "ax13.spines['left'].set_color('darkgrey')\n",
    "ax13.spines['right'].set_color('darkgrey')\n",
    "[ax13.spines[i].set_linewidth(2) for i in ax13.spines]\n",
    "ax13.get_xaxis().set_visible(False)\n",
    "ax13.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "for i,(proj_point,colour,label, offset) in enumerate(zip(proj_point_list,colours,labels,offsets)):\n",
    "    max_dist = proj_point_list[i].max()\n",
    "    print(max_dist)\n",
    "    edges = np.arange(0,max_dist,dx)\n",
    "    profile_stats = np.empty(shape=(edges.shape[0]-1, 3), dtype=float) # pre-alloc nd-array\n",
    "    for bin_id in enumerate(edges[:-1]):\n",
    "        sel = np.squeeze(np.asarray([(proj_point[:,2]>=edges[bin_id[0]]) & (proj_point[:,2]<=edges[bin_id[0]+1])]))\n",
    "        profile_stats[bin_id[0],:] = [np.nanpercentile(proj_point[sel,3],2.5),\n",
    "                                np.nanmean(proj_point[sel,3]),\n",
    "                                np.nanpercentile(proj_point[sel,3],97.5)]\n",
    "\n",
    "\n",
    "    print(profile_stats)\n",
    "\n",
    "\n",
    "    ax2.fill_between(edges[:-1]+dx/2 , profile_stats[:,0]+offset,profile_stats[:,2]+offset,alpha=0.3,color=colour)\n",
    "    ax2.scatter(edges[:-1]+dx/2, profile_stats[:,1]+offset,marker='.',color=colour,label=label)\n",
    "    ax2.set_xlabel('Distance along profile [m]')\n",
    "    ax2.set_ylabel('Range offset [m]')\n",
    "    # ax0.scatter(proj_point[:,2],proj_point[:,3],marker='.')\n",
    "    ax2.legend(borderpad=0.1,labelspacing=0.2,ncol=2,frameon=False)\n",
    "    ax2.set_ylim([-0.1,4.5])\n",
    "    # plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
